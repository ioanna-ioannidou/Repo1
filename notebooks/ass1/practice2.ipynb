{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A healthcare organization is interested in understanding the relationship between the number of visits to the doctors office and certain patient characteristics. They have collected data on the number of visits (for a year) for a sample of patients and have included the following variables\n",
    "- **ofp**: number of physician office visits\n",
    "- **ofnp**: number of nonphysician office visits\n",
    "- **opp**: number of physician outpatient visits\n",
    "- **opnp**: number of nonphysician outpatient visits\n",
    "- **emr**: number of emergency room visits\n",
    "- **hosp**: number of hospitalizations\n",
    "- **exclhlth**: the person is of excellent health (self-perceived)\n",
    "- **poorhealth**: the person is of poor health (self-perceived)\n",
    "- **numchron**: number of chronic conditions\n",
    "- **adldiff**: the person has a condition that limits activities of daily living\n",
    "- **noreast**: the person is from the northeast region\n",
    "- **midwest**: the person is from the midwest region\n",
    "- **west**: the person is from the west region\n",
    "- **age**: age in years (divided by 10)\n",
    "- **male**: is the person male?\n",
    "- **married**: is the person married?\n",
    "- **school**: number of years of education\n",
    "- **faminc**: family income in 10000$\n",
    "- **employed**: is the person employed?\n",
    "- **privins**: is the person covered by private health insurance?\n",
    "- **medicaid**: is the person covered by Medicaid?\n",
    "- Decide which patient features are resonable to use to predict the target \"number of physician office visits\". \n",
    "- Hint: should we really use the \"ofnp\" etc variables?\n",
    "- Since the target variable is counts, a reasonable loss function is to consider the target variable as Poisson distributed where the parameter follows ùúÜ = exp(ùõº ‚ãÖ ùë• + ùõΩ) where ùõº is a vector (slope) and ùõΩ is a number (intercept). That is, the parameter is the exponential of a linear function. The reason we chose this as our parameter, is that it is always positive which is when the Poisson distribution is defined. To be specific we make the following assumption about our conditional density of ùëå ‚à£ ùëã,\n",
    "$$\n",
    "f_{Y|X}(y, x) = \\lambda ^y e^{-\\lambda}, \\quad \\lambda(x) = \\exp(\\alpha \\cdot x + \\beta)\n",
    "$$\n",
    "- Recall from the lecture notes, (4.2) that in this case we should consider the log-loss (entropy) and that according to (4.2.1 Maximum Likelihood and regression) we can consider the conditional log-likelihood. Follow the steps of Example 1 and Example 2 in section (4.2) to derive the loss that needs to be minimized.\n",
    "- Hint: when taking the log of the conditional density you will find that the term that contains the ùë¶! does not depend on ùúÜ and as such does not depend on ùõº, ùõΩ, it can thus be discarded. This will be essential due to numerical issues with factorials.\n",
    "Instructions:\n",
    "1. Load the file data/visits_clean.csv , follow the instructions in the code cell of how this should happen \n",
    "2. Create the problem2_X and the problem2_y as numpy arrays with problem2_X being the features and problem2_y being the target. Do the standard train-test split with 80% training data and 20% testing data. Store these in the variables defined in the cells.\n",
    "3. Implement ùëôùëúùë†ùë† inside the class PoissonRegression by writing down the loss to be minimized, I have provided a formula for the ùúÜ that you can use.\n",
    "4. Now use the PoissonRegression class to train a Poisson regression model on the training data. \n",
    "5. Compute the mean absolute error of your prediction on the test set and use Hoeffdings inequality to produce a 95\\% confidence interval for the mean absolute error. We can make the assumption that the error is bounded by 70 for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 6.9, 1.0, 1.0, 6.0, 2.881, 1.0, 1.0, 0.0], [1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 7.4, 0.0, 1.0, 10.0, 2.7478, 0.0, 1.0, 0.0], [13.0, 0.0, 0.0, 0.0, 3.0, 3.0, 0.0, 1.0, 4.0, 1.0, 0.0, 0.0, 0.0, 6.6, 0.0, 0.0, 10.0, 0.6532, 0.0, 0.0, 1.0], [16.0, 0.0, 5.0, 0.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 7.6, 1.0, 1.0, 3.0, 0.6588, 0.0, 1.0, 0.0], [3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 7.9, 0.0, 1.0, 6.0, 0.6588, 0.0, 1.0, 0.0]]\n",
      "4406\n",
      "21\n",
      "['ofp', 'ofnp', 'opp', 'opnp', 'emr', 'hosp', 'exclhlth', 'poorhlth', 'numchron', 'adldiff', 'noreast', 'midwest', 'west', 'age', 'male', 'married', 'school', 'faminc', 'employed', 'privins', 'medicaid']\n"
     ]
    }
   ],
   "source": [
    "#Part1\n",
    "# As in assignment 1 we will load the header into header and data into data\n",
    "# this time you will have to parse the data such that each data entry is a float\n",
    "# and that the problem2_data is a numpy array of shape (n_samples,n_columns)\n",
    "# where n_columns is the number of columns and should have the samelength as\n",
    "# the list of strings header. n_samples is how many rows of data wehad.\n",
    "# If you cannot find the file, check the starting package as it should be updated\n",
    "# if not, go to the github repo and pull it\n",
    "# The autograder does not accept pandas as a solution to this probl em.\n",
    "# data/visits_clean.csv\n",
    "import csv \n",
    "import numpy as np\n",
    "header = []\n",
    "data = []\n",
    "\n",
    "with open('data/visits_clean.csv', 'r') as datafile:\n",
    "    csv_read = csv.reader(datafile, delimiter=' ')  # Specify space as delimite)\n",
    "\n",
    "    header = next(csv_read)\n",
    "\n",
    "    # Parse the data and convert each entry to a float\n",
    "    data = [[float(value) for value in row] for row in csv_read]\n",
    "head = data[:5]\n",
    "print(head)\n",
    "data_array = np.array(data)\n",
    "num_rows, num_columns = data_array.shape\n",
    "print(num_rows)\n",
    "print(num_columns)\n",
    "print(header)\n",
    "\n",
    "problem2_header = ['ofp', 'ofnp', 'opp', 'opnp', 'emr', 'hosp', 'exclhlth', 'poorhlth', 'numchron', 'adldiff', 'noreast', 'midwest', 'west', 'age', 'male', 'married', 'school', 'faminc', 'employed', 'privins', 'medicaid'] #List of strings\n",
    "problem2_data = np.array(data) #A numpy array of shape n_samples n_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4406, 15)\n",
      "y shape: (4406,)\n",
      "X_train shape: (3525, 15)\n",
      "X_test shape: (881, 15)\n",
      "y_train shape: (3525,)\n",
      "y_test shape: (881,)\n"
     ]
    }
   ],
   "source": [
    "#Part2\n",
    "# Fill in your X and y below\n",
    "# lets keep exclhlth, poorhealth, numchron, adldiff, age, male, married, school, faminc, employed, privins, medicaid, (noreast, midwest, west: optional)\n",
    "import numpy as np\n",
    "# List of relevant features for prediction\n",
    "relevant_features = ['exclhlth', 'poorhlth', 'numchron', 'adldiff', 'age', 'male', \n",
    "                     'married', 'school', 'faminc', 'employed', 'privins', 'medicaid', \n",
    "                     'noreast', 'midwest', 'west']\n",
    "\n",
    "# Get the indices of the relevant features and the target variable\n",
    "target_index = header.index('ofp')\n",
    "relevant_indices = [header.index(var) for var in relevant_features]\n",
    "\n",
    "# Split the data into X (features) and y (target variable)\n",
    "y = data_array[:, target_index]  # Target variable (ofp)\n",
    "X = data_array[:, relevant_indices]  # Relevant features\n",
    "\n",
    "# Now X contains only the relevant features, and y contains the target variable\n",
    "print(\"X shape:\", X.shape)  # Check the shape of X\n",
    "print(\"y shape:\", y.shape)  # Check the shape of y\n",
    "\n",
    "problem2_X = y\n",
    "problem2_y = X\n",
    "# Split the data into train and randomly using for instance\n",
    "# np.random.shuffle indices and indexing the first 80% as the train data\n",
    "# keep the train size as 0.8 rounded up to the nearest integer sample\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "# Calculate the size of the training set (80% of total samples, rounded up)\n",
    "train_size = int(np.ceil(0.8 * n_samples))\n",
    "\n",
    "# Generate a random permutation of indices (shuffling)\n",
    "indices = np.random.permutation(n_samples)\n",
    "\n",
    "# Split the indices into train and test sets\n",
    "train_indices = indices[:train_size]\n",
    "test_indices = indices[train_size:]\n",
    "\n",
    "problem2_X_train, problem2_X_test, problem2_y_train, problem2_y_test = X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "print(\"X_train shape:\", problem2_X_train.shape)\n",
    "print(\"X_test shape:\", problem2_X_test.shape)\n",
    "print(\"y_train shape:\", problem2_y_train.shape)\n",
    "print(\"y_test shape:\", problem2_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " to maximise the negative log likelihood we need to minimise it:\n",
    " logf Y‚à£X(y,x)=y(Œ±‚ãÖx+Œ≤)‚àíexp(Œ±‚ãÖx+Œ≤)\n",
    " $$ L(\\alpha, \\beta) = \\sum_{i=1}^{n} \\left[ \\exp(\\alpha \\cdot x_i + \\beta) - y_i (\\alpha \\cdot x_i + \\beta) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part3\n",
    "# Fill in the function loss below\n",
    "# logf Y‚à£X(y,x)=y(Œ±‚ãÖx+Œ≤)‚àíexp(Œ±‚ãÖx+Œ≤)\n",
    "# $$ L(\\alpha, \\beta) = \\sum_{i=1}^{n} \\left[ \\exp(\\alpha \\cdot x_i + \\beta) - y_i (\\alpha \\cdot x_i + \\beta) \\right]$$\n",
    "\n",
    "class PoissonRegression(object): \n",
    "    def __init__(self):\n",
    "        self.coeffs = None \n",
    "        self.result = None\n",
    "    # define the objective/cost/loss function we want to minimise\n",
    "    def loss(self,X,Y,coeffs):\n",
    "# The parameter lambda for the given X and the proposed values\n",
    "# of the coefficients, here coeff[:-1] represent alpha # and coeff[-1] represent beta\n",
    "# Extract coefficients\n",
    "        alpha = coeffs[:-1]  # alpha (slope)\n",
    "        beta = coeffs[-1]    # beta (intercept)\n",
    "        \n",
    "# Compute lambda for each sample\n",
    "        lam = np.exp(np.dot(X, alpha) + beta)\n",
    "        \n",
    "    # Compute the log-likelihood (Poisson log-loss)\n",
    "    # We ignore the factorial term as per the problem statement\n",
    "        loss = np.sum(lam - Y * np.log(lam))\n",
    "    \n",
    "# use the Y variable that is available here to define # the loss function, return the value of the loss for # this Y and for this parameter lam defined above \n",
    "        return loss\n",
    "   \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Use the loss function together with an optimization method from scipy\n",
    "        opt_loss = lambda coeffs: self.loss(X, Y, coeffs) \n",
    "        initial_arguments = np.zeros(shape=X.shape[1] + 1)  # initial guess as 0\n",
    "        \n",
    "        # Minimize the loss function using 'cg' (Conjugate Gradient) method\n",
    "        self.result = optimize.minimize(opt_loss, initial_arguments, method='cg')\n",
    "        \n",
    "        # Store the optimized coefficients\n",
    "        self.coeffs = self.result.x \n",
    "    def predict(self,X):\n",
    "#Use the trained model to predict Y \n",
    "        if (self.coeffs is not None):\n",
    "            return np.exp(np.dot(X,self.coeffs[:-1])+self.coeffs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization result:\n",
      " message: Desired error not necessarily achieved due to precision loss.\n",
      " success: False\n",
      "  status: 2\n",
      "     fun: -16112.663818423369\n",
      "       x: [-3.823e-01  2.624e-01 ...  1.005e-01  1.180e+00]\n",
      "     nit: 640\n",
      "     jac: [ 1.221e-04 -1.099e-03 ... -1.587e-03  7.690e-03]\n",
      "    nfev: 24152\n",
      "    njev: 1420\n",
      "Optimization failed. Try adjusting the optimization settings.\n"
     ]
    }
   ],
   "source": [
    "#Part4\n",
    "# Initialize your PoissonRegression model\n",
    "problem2_model = PoissonRegression()\n",
    "# Fit your initialized model on the training data\n",
    "# This is to make sure that everything went well, \n",
    "# # check that success is True \n",
    "# print(problem2_model.result)\n",
    "\n",
    "# Fit the model on the training data (example: problem2_X_train, problem2_y_train)\n",
    "problem2_model.fit(problem2_X_train, problem2_y_train)\n",
    "\n",
    "print(\"Optimization result:\")\n",
    "print(problem2_model.result)\n",
    "\n",
    "# Check if optimization was successful\n",
    "if problem2_model.result.success:\n",
    "    print(\"Optimization successful!\")\n",
    "    print(\"Fitted coefficients:\", problem2_model.coeffs)\n",
    "else:\n",
    "    print(\"Optimization failed. Try adjusting the optimization settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what to do in such case: \n",
    "1. Standardisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "2. Use a Different Optimization Method\n",
    "self.result = optimize.minimize(opt_loss, initial_arguments, method='BFGS')\n",
    "self.result = optimize.minimize(opt_loss, initial_arguments, method='L-BFGS-B')\n",
    "\n",
    "3. Check multicolinearity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "correlation_matrix = pd.DataFrame(X).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "4. Adjust tolerance and max iterations \n",
    "self.result = optimize.minimize(\n",
    "    opt_loss,\n",
    "    initial_arguments,\n",
    "    method='BFGS',\n",
    "    options={'maxiter': 1000, 'gtol': 1e-6, 'disp': True}\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 4.435330034324141\n",
      "95% Confidence Interval: (1.2324378497946964, 7.638222218853587)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Part5\n",
    "import numpy as np\n",
    "predictions = problem2_model.predict(problem2_X_test)\n",
    "MAE = np.mean(np.abs(problem2_y_test - predictions))\n",
    "# Put the computed mean absolute error in the variable below\n",
    "problem2_metric = MAE\n",
    "# Put a confidence interval in the variable below by using Hoeffdin gs inequality using the bounds\n",
    "# a = 0, b=70 (roughly 5 days between visits as minimum)\n",
    "# the variable should contain a tuple representing the confidence i nterval of the form (l_edge,r_edge)\n",
    "# Hoeffding's Inequality to compute 95% confidence interval\n",
    "# Define bounds for the errors\n",
    "a = 0\n",
    "b = 70\n",
    "# Sample size (number of test samples)\n",
    "n = len(problem2_y_test)\n",
    "\n",
    "# Compute epsilon for the 95% confidence interval\n",
    "epsilon = np.sqrt((b - a) ** 2 * np.log(2 / 0.05) / (2 * n))\n",
    "\n",
    "# Confidence interval (l_edge, r_edge)\n",
    "lower_bound = MAE - epsilon\n",
    "upper_bound = MAE + epsilon\n",
    "problem2_interval = (lower_bound, upper_bound)\n",
    "\n",
    "problem2_interval = problem2_interval\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {problem2_metric}\")\n",
    "print(f\"95% Confidence Interval: {problem2_interval}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random variable generation and transformation\n",
    "The purpose of this problem is to show that you can implement your own sampler, this will be built in the following three steps:\n",
    "1. Implement a Linear Congruential Generator where you tested out a good combination (a large ùëÄ with ùëé, ùëè satisfying the Hull-Dobell (Thm 6.8)) of parameters. Follow the instructions in the code block.\n",
    "2. Using a generator construct random numbers from the uniform [0, 1] distribution. \n",
    "3. Using a uniform [0, 1] random generator, generate samples from\n",
    "$$\n",
    "p_0(x) = \\pi \\lvert \\sin(2\\pi x) \\rvert, \\quad x \\in [0, 1].\n",
    "$$\n",
    "Using the Accept-Reject sampler (Algorithm 1 in TFDS notes) with sampling density given by the uniform [0, 1] distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5823075897060335, 0.5198187492787838, 0.46597642498090863, 0.7770372582599521, 0.42286502895876765, 0.03337232954800129, 0.41738913068547845, 0.8087285170331597, 0.6123396842740476, 0.7149040475487709]\n"
     ]
    }
   ],
   "source": [
    "def problem3_LCG(size=None, seed=0):\n",
    "    \"\"\"\n",
    "    A linear congruential generator that generates pseudo-random numbers according to size.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    size : int\n",
    "        An integer denoting how many samples should be produced.\n",
    "    seed : int\n",
    "        The starting point of the LCG, i.e., u0 in the notes.\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    out : list\n",
    "        A list of the pseudo-random numbers.\n",
    "    \"\"\"\n",
    "    # Define LCG parameters satisfying the Hull-Dobell theorem\n",
    "    M = 2**31  # Large modulus\n",
    "    a = 1103515245  # Multiplier, satisfies conditions for M\n",
    "    b = 12345  # Increment, gcd(b, M) = 1\n",
    "\n",
    "    # Initialize the sequence with the seed\n",
    "    u = seed\n",
    "    random_numbers = []\n",
    "\n",
    "    # Generate `size` random numbers\n",
    "    for _ in range(size):\n",
    "        u = (a * u + b) % M\n",
    "        random_numbers.append(u / M)  # Normalize to [0, 1)\n",
    "\n",
    "    return random_numbers\n",
    "\n",
    "# Example usage\n",
    "random_samples = problem3_LCG(size=10, seed=42)\n",
    "print(random_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hull-Dobell Theorem gives the conditions under which a Linear Congruential Generator (LCG) has the maximum possible period, which is equal to the modulus \\( M \\). To understand this, let's break it down step-by-step with a simple numerical example.\n",
    "\n",
    "### The LCG Equation:\n",
    "The general formula is:\n",
    "\\[\n",
    "u_{n+1} = (a \\cdot u_n + b) \\, \\text{mod} \\, M\n",
    "\\]\n",
    "Where:\n",
    "- \\( u_n \\) is the current state,\n",
    "- \\( a \\) is the multiplier,\n",
    "- \\( b \\) is the increment,\n",
    "- \\( M \\) is the modulus.\n",
    "\n",
    "### Hull-Dobell Conditions:\n",
    "For the LCG to have a full period of \\( M \\), the following must be true:\n",
    "\n",
    "1. \\( \\text{gcd}(b, M) = 1 \\):\n",
    "   The greatest common divisor (GCD) of \\( b \\) and \\( M \\) must be 1. This ensures that the sequence can traverse all possible values modulo \\( M \\).\n",
    "\n",
    "2. Every prime \\( p \\) that divides \\( M \\) must also divide \\( a - 1 \\):\n",
    "   This condition ensures the generator \"cycles through\" values in a way compatible with the prime factors of \\( M \\).\n",
    "\n",
    "3. If \\( M \\) is divisible by 4, \\( a - 1 \\) must also be divisible by 4:\n",
    "   This additional condition ensures compatibility with powers of 2 in \\( M \\).\n",
    "\n",
    "### Example:\n",
    "Let‚Äôs take simple values for \\( M = 8 \\), \\( a = 5 \\), and \\( b = 3 \\). We‚Äôll test these parameters.\n",
    "\n",
    "#### Step 1: Verify the Hull-Dobell Conditions\n",
    "\n",
    "1. \\( \\text{gcd}(b, M) = \\text{gcd}(3, 8) = 1 \\) ‚úîÔ∏è\n",
    "\n",
    "2. The prime factors of \\( M = 8 \\) are \\( 2 \\). Check \\( a - 1 = 5 - 1 = 4 \\). Since \\( 2 \\) divides \\( 4 \\), this condition is satisfied. ‚úîÔ∏è\n",
    "\n",
    "3. Since \\( M = 8 \\) is divisible by 4, we also check that \\( a - 1 = 4 \\) is divisible by 4. ‚úîÔ∏è\n",
    "\n",
    "All conditions are satisfied, so this LCG should have a full period of 8.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.711581018315784e-10, 2.420594679558574e-10, 2.169871819116681e-10, 3.6183616996740553e-10, 1.9691187374236419e-10, 1.5540201937780385e-11, 1.943619599032581e-10, 3.765935623241401e-10, 2.8514288564866763e-10, 3.329031390830739e-10]\n"
     ]
    }
   ],
   "source": [
    "def problem3_uniform(generator=None, period=1, size=None, seed=0):\n",
    "    \"\"\"\n",
    "    Takes a generator and produces samples from the uniform [0,1] distribution according\n",
    "    to size.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    generator : function\n",
    "        A function of type generator(size, seed) that produces pseudo-random numbers in the range {0, 1, ..., period-1}.\n",
    "    period : int\n",
    "        The period of the generator.\n",
    "    seed : int\n",
    "        The seed to be used in the generator provided.\n",
    "    size : int\n",
    "        An integer denoting how many samples should be produced.\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    out : list\n",
    "        A list of the uniform pseudo-random numbers.\n",
    "    \"\"\"\n",
    "    if generator is None:\n",
    "        raise ValueError(\"A valid generator function must be provided.\")\n",
    "\n",
    "    # Generate raw pseudo-random numbers using the provided generator\n",
    "    raw_numbers = generator(size=size, seed=seed)\n",
    "\n",
    "    # Normalize raw numbers to the [0, 1] range by dividing by the period\n",
    "    uniform_numbers = [num / period for num in raw_numbers]\n",
    "\n",
    "    return uniform_numbers\n",
    "\n",
    "# Example usage with the LCG generator\n",
    "uniform_samples = problem3_uniform(generator=problem3_LCG, period=2**31, size=10, seed=42)\n",
    "print(uniform_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.711581018315784e-10, 2.420594679558574e-10, 2.169871819116681e-10, 3.6183616996740553e-10, 1.9691187374236419e-10, 1.5540201937780385e-11, 1.943619599032581e-10, 3.765935623241401e-10, 2.8514288564866763e-10, 3.329031390830739e-10, 8.488042408695551e-11, 2.4037198924006797e-10, 2.6168622455600976e-10, 1.1580264438343368e-10, 4.2822442922370307e-10, 5.162491831592342e-11, 3.0658076316351923e-10, 1.5295425516426153e-10, 8.62819262651554e-11, 3.124615926240748e-10, 4.2782521427441045e-10, 6.1228665367008e-11, 1.9120661933109961e-10, 3.9540949377595003e-10, 1.907466135996766e-10, 1.5226431226977866e-11, 2.636377351237368e-10, 3.743977068475235e-10, 6.899785582291373e-11, 4.204851562425449e-10, 2.235359651287655e-10, 2.1002019784735482e-10, 3.280961047551906e-10, 3.8888215390942005e-10, 3.628417811433332e-10, 2.9085569152806356e-10, 1.0450256437109784e-10, 2.985778161171404e-10, 4.2955742717183665e-10, 1.1487088407216572e-10, 4.173602847871993e-11, 4.4949822683437723e-10, 1.3700710899990087e-10, 4.4006253068634704e-11, 4.3631108773665994e-10, 1.6537967436475176e-10, 3.8523153525656106e-10, 1.2200347546467705e-10, 1.176385610018177e-10, 6.358320987776e-11, 5.7690734351148465e-11, 4.2291242860134637e-10, 1.2187249972227254e-10, 2.0216972280301393e-10, 2.5308605840386467e-10, 1.7996579617166053e-10, 1.5437370197261824e-10, 8.507269194657496e-11, 1.0526569047854262e-10, 1.6225925420984555e-10, 2.8267725638540797e-10, 2.1977839253367604e-10, 2.419761520062319e-10, 4.266510985652394e-10, 4.558563834571039e-11, 3.247982265086602e-11, 3.6373056324680297e-10, 2.406257255081756e-10, 4.3535556908634587e-10, 2.1701730083117932e-10, 8.636844061118976e-12, 2.1977671115294695e-10, 7.277585435325196e-12, 1.3162041248571121e-10, 1.6032750322671208e-10, 4.3713785672847005e-10, 3.525035643155844e-10, 1.8240940615615875e-10, 3.2933429702959593e-10, 1.3595339307462262e-10, 8.122767974731715e-11, 2.7333200459944695e-10, 4.0658158805863265e-10, 2.536963083187016e-10, 7.054214590934693e-11, 5.66965357041288e-11, 4.1908737287779663e-10, 4.585849820541721e-11, 7.7138503917773e-11, 2.931970762096864e-10, 1.1826997280833806e-11, 3.6337769577197976e-10, 4.187048442336194e-10, 6.906665734121575e-11, 2.0022149107949694e-10, 6.086036362373815e-11, 4.460040221258149e-10, 2.615095410184176e-10, 4.5084750321076894e-10, 3.4418137871000676e-10, 2.4752535719881064e-10, 2.9602190880842483e-10, 2.2712860476940822e-10, 1.9088003053169267e-10, 4.3576178277750233e-10, 4.3330610930911173e-10, 3.4484401770749906e-10, 3.727787028715013e-10, 1.7583602044020374e-10, 2.32968421030183e-10, 3.263826802140518e-10, 4.2070644906168353e-10, 4.5908098286404075e-10, 4.302238303024314e-10, 3.197041910721343e-10, 3.2861063111074007e-10, 2.736516746711104e-10, 4.2715504115602154e-10, 3.009931262565324e-10, 4.0258417975148897e-10, 2.390366580888592e-10, 1.1453463156195498e-10, 1.3903368994289123e-10, 4.631402251726452e-10, 4.627417151716051e-10, 2.734779961082595e-10, 3.5794951551426646e-10, 3.5579944914800044e-10, 2.1469550920069636e-10, 2.1271031962720455e-10, 1.6479977972549964e-11, 3.5212040509942366e-10, 4.5763437527338025e-10, 3.828522540227225e-10, 4.3480526470962566e-10, 2.9835563381853947e-10, 2.5385928385454615e-10, 1.5128428696407903e-10, 1.7112292052118283e-10, 5.831873981128333e-11, 1.3053676759314237e-10, 1.5267895519914532e-10, 2.6732907033866216e-10, 3.0458793842148835e-10, 1.0717544343327723e-10, 2.3666920658492463e-10, 3.6610132026632103e-10, 1.5103450369709226e-10, 3.4517575993667227e-10, 9.94356270499902e-11, 6.212366732150088e-11, 4.596886606176437e-10, 3.932648607804512e-11, 1.3993885889483637e-10, 5.958903769726076e-11, 1.828391348913938e-10, 3.143541804900146e-10, 2.692521535591075e-10, 2.9036350776036335e-10, 2.3785839053588886e-10, 1.173700214709278e-10, 1.273779562729893e-10, 2.1225944201071212e-10, 3.966364671599554e-10, 2.578751073355895e-10, 1.3250343530724074e-10, 4.6558668921094226e-10, 3.966320427477299e-10, 3.101315486971762e-10, 1.0347105724312078e-10, 3.286432178912363e-10, 5.546367401812469e-11, 3.5853916602150704e-10, 1.3957095375272122e-10, 2.481967619708675e-10, 4.0522021979225165e-10, 4.5142707779353974e-10, 2.519113502866266e-10, 2.2143690288529971e-10, 3.475098581291747e-10, 2.940423952501464e-10, 4.5590280856044885e-10, 1.0558975785737723e-11, 2.8160498195470285e-10, 4.4675264854708574e-10, 8.79434884290542e-11, 1.1274666313412784e-11, 1.7648396155936497e-10, 8.05119057360748e-12, 4.6449743877630123e-10, 4.323909461815406e-10, 2.794980475361031e-10, 4.302853748219121e-10, 3.334273768543228e-10, 4.017199379136721e-10, 3.143675740731322e-10, 2.031484327546379e-10, 3.429674357013912e-10, 2.4339680466424485e-10, 2.38961575787372e-10, 2.9740316784786225e-10, 3.902618913795286e-10, 1.5996508393074926e-10, 2.782101272448545e-10, 4.4726551867539655e-10, 2.125861466029111e-11, 4.269517103142745e-10, 4.297450719933926e-11, 1.1031937321125118e-10, 1.7938316543980592e-10, 5.858144459976178e-11, 3.815994438841064e-11, 1.5874455179185065e-11, 1.7663721874061622e-10, 4.4933681623594846e-10, 2.4862188653315687e-10, 4.4550974693212397e-10, 2.9174071058263307e-10, 2.398036884083271e-10, 3.747312399618452e-11, 7.295615392192978e-11, 1.4367109411883572e-10, 1.8702022786323824e-11, 9.893944474467786e-11, 3.546253340893507e-10, 4.512513704715837e-10, 3.1557422929158474e-10, 4.558228313029933e-10, 2.058256176173251e-10, 1.897854338961391e-10, 3.201597825828323e-10, 3.659940367266301e-10, 3.894718469173858e-10, 1.502140686143738e-11, 3.407025575292294e-10, 1.8157028528267838e-10, 2.4023625753641364e-11, 2.4015120621279085e-10, 7.03970842123175e-11, 3.149903267905818e-10, 4.46507221604428e-10, 4.5175430236910064e-11, 3.308330478058591e-10, 1.4961031415475223e-10, 2.3433947208932604e-10, 1.7418042702610492e-10, 3.6164423387365086e-10, 4.3093590458218034e-10, 3.470992380235488e-10, 4.3303415280665503e-10, 1.87866393665595e-10, 3.3458187132309747e-10, 3.9799703008096265e-10, 2.966444156288217e-10, 1.3421022995209475e-10, 1.9765522335166152e-10, 3.59575478550353e-10, 4.575441353918408e-10, 3.283466836097315e-11, 1.7833054520924319e-10, 3.5492345585967644e-10, 1.2076988367692998e-10, 4.3837493552724427e-10, 2.423781067344144e-10, 2.6608176404395445e-10, 4.4965309167060985e-10, 1.4907437480629615e-11, 1.2331662644152197e-10, 1.9282636837085476e-10, 1.1992573080432667e-10, 2.561704184281949e-10, 4.639996525022949e-10, 2.3387807814544137e-10, 3.7167916400876466e-10, 6.219021066351804e-11, 6.101714663045432e-11, 1.5806336860040016e-10, 3.943038118237041e-10, 1.8119651460681008e-10, 1.9860826828629885e-10, 2.314429687396563e-10, 4.339483312618131e-10, 3.997651970739924e-11, 2.832573364232316e-10, 4.2642610579776696e-11, 4.4354526128331795e-10, 4.5229328290465054e-10, 4.2228183927059404e-10, 3.1769286311898737e-10, 2.34096500864589e-11, 3.164536230716025e-10, 2.7985862975990483e-10, 3.75660860708546e-10, 4.531237537963584e-11, 3.2598966082965364e-10, 7.49101436263444e-11, 4.073324349259527e-10, 6.650249014666931e-11, 4.0755540023536263e-10, 3.921608740867222e-11, 2.9690488500926093e-10, 3.0848347661038833e-10, 1.847121832657811e-10, 9.220847609764382e-12, 4.43658597490057e-10, 4.4437259167501297e-10, 3.575753089891248e-10, 1.2651494131835084e-10, 2.885804854628471e-11, 2.443578880906294e-10, 3.271570291150244e-10, 1.2950917378448668e-10, 4.749554894344088e-11, 3.7667882962083565e-10, 1.9892653258142545e-10, 7.392497248029373e-11, 3.087024277696748e-11, 3.174191291754891e-11, 1.6611569433368223e-10, 4.302808504462463e-10, 4.3622859296175986e-10, 3.7700682246205597e-10, 2.8501704056778376e-10, 2.0504688268488391e-10, 9.512015198935575e-11, 2.657062105060337e-10, 1.6206072790160564e-10, 3.4687289455700987e-10, 2.3103356359098493e-10, 1.9195393798771e-10, 1.679331268229084e-10, 3.743526378642559e-10, 2.470150080573913e-10, 4.496435810491528e-10, 4.388947957238015e-10, 1.4334252665046399e-10, 5.34630480077819e-11, 1.7446629297507288e-10, 2.118830104858721e-10, 2.78232656098637e-10, 1.9039882235942496e-10, 4.5591745656548e-10, 2.728146582340468e-10, 1.4667263193920976e-10, 7.890563875033452e-11, 3.783101176074721e-11, 3.544650521453835e-10, 4.0632726567083055e-10, 9.153681870648078e-11, 2.9848822545586184e-10, 3.1693521093147103e-10, 1.5560020416235942e-10, 3.258052488821356e-10, 1.125979040908498e-10, 2.067121710348091e-10, 3.735134220146652e-10, 3.7356963074157423e-10, 8.016253980058785e-11, 1.1109299092627867e-10, 4.2105163409675295e-10, 4.2101147893457147e-10, 4.4981768917290443e-10, 2.625117592920667e-11, 2.2048213125028249e-10, 5.5775976502345226e-11, 1.1446560539696282e-10, 1.9005489564939693e-10, 7.233256962141388e-12, 2.382448975515178e-10, 9.890320246813689e-11, 2.3527057320567313e-10, 2.6293707228869367e-11, 1.569018324553556e-10, 2.9081387992180296e-10, 4.082289421867419e-10, 4.4961904295190425e-10, 1.269337391272829e-11, 2.8540388368608616e-10, 4.1461942668248597e-10, 1.6121729862553225e-10, 5.536263700083033e-11, 7.997204114207346e-11, 6.881251493097429e-11, 2.743355243493839e-10, 3.9740074924375646e-10, 3.1282265059578984e-10, 1.0234370577573426e-10, 3.172095368493555e-10, 3.1049585537228086e-10, 1.1111679978914601e-10, 3.2303938885848427e-10, 4.0237931346262523e-10, 1.3314141087371334e-10, 1.5831377788572132e-10, 4.6533754735796073e-10, 1.1000002861707991e-10, 3.0333172323752933e-10, 1.982333967115446e-10, 4.5762235689229824e-10, 4.152318571447322e-10, 1.7934522768790762e-10, 4.490360349176936e-10, 3.248514090104654e-10, 3.2146111857492277e-10, 1.0107915155918579e-11, 3.3510280661452896e-10, 1.7758493135212883e-10, 1.1737133443975867e-10, 3.362342277431901e-10, 2.848223696824692e-10, 4.0034445593709056e-11, 1.2564904889114659e-10, 3.2059269019883707e-10, 4.339015225677395e-10, 3.420849623535227e-10, 4.653644880472235e-10, 3.577467358809039e-10, 2.1099506451911776e-10, 7.740354928190141e-11, 3.857136550260154e-11, 4.4258356354798245e-10, 2.0661398590290925e-10, 2.549765015010669e-10, 1.9271716774488246e-10, 3.236538201074196e-10, 1.5329624939233732e-10, 2.747057258317004e-10, 1.2648736398557237e-10, 3.8382998884291253e-10, 1.2985709859844596e-10, 3.2412174485151046e-10, 2.1930471067604929e-10, 1.303563708799499e-10, 2.955085104568156e-10, 3.99282600689263e-10, 1.7075773434127584e-10, 4.074996228040778e-10, 3.680839725898888e-10, 8.95257457576848e-12, 2.92905036163027e-10, 4.095620132101019e-11, 1.9900966508404339e-10, 4.560128028224114e-10, 1.530249798403771e-10, 8.149779831025106e-11, 1.9093305885994893e-10, 2.1307862223783613e-10, 2.5316818866999435e-10, 5.870582080354236e-11, 2.0468081439808938e-10, 2.579933946599694e-10, 1.5449525339605863e-10, 2.0008161989194806e-10, 2.60000940048577e-11, 1.4611879393076899e-10, 1.9139002795792726e-10, 5.492416352455287e-11, 3.875517138110518e-11, 2.8810744306766167e-10, 1.8333091468536455e-10, 2.76812602787585e-11, 5.2759291293419386e-11, 5.764234380610522e-11, 4.268653668385025e-10, 4.5111761483480894e-10, 5.518782349514528e-11, 2.1290967665114902e-10, 2.383151592733057e-10, 4.73929491571351e-11, 3.6809137638968425e-10, 1.8761198822790648e-10, 2.608802297451862e-10, 4.8925652808631814e-11, 3.24342041505693e-10, 1.322722059920317e-10, 8.147834316962754e-11, 4.60312715245062e-10, 3.9511203120054517e-10, 1.242806463210705e-10, 1.6495610519889903e-10, 3.2111685120857214e-10, 3.619598583533279e-10, 3.7383872104716775e-10, 1.166698647414588e-10, 9.232375476099508e-11, 3.292684696946935e-10, 4.321869561448727e-10, 4.1842688255216975e-10, 3.3115147538183287e-10, 4.29739492689013e-10, 2.9794845301904516e-10, 1.1444659282766612e-10, 3.6495029936446655e-10, 4.307525599232809e-10, 3.09915441833869e-11, 3.8630005878143026e-10, 4.421008760902706e-10, 2.1656803130334912e-10, 3.0398148646686163e-10, 4.398907141323072e-10, 3.604393643361764e-10, 2.587564147324417e-11, 3.536365623078852e-11, 3.0004736455840897e-10, 3.794237747340584e-10, 2.2683530834927135e-11, 5.320611204222281e-11, 1.9542155220431123e-10, 2.1053800738392303e-10, 4.083566241229464e-10, 8.60795304393619e-11, 4.1905064921549107e-10, 2.95026115083169e-11, 2.5581495298812595e-10, 3.5870881677328714e-10, 1.9484324223495442e-10, 2.3611134076541307e-10, 4.646524723143919e-10, 2.1084855714691153e-10, 3.6667074845148084e-10, 1.799722326462777e-10, 1.7062250787583388e-10, 3.6050374035805077e-10, 5.841865359512699e-11, 2.8552882497603904e-11, 3.9015355659741124e-10, 2.9862534125200957e-10, 3.7923196700984096e-11, 4.078731929025442e-10, 4.238999125674714e-11, 2.3036991823703612e-10, 1.3917916819039533e-10, 2.669549704990143e-10, 4.5772782742911633e-10, 4.470366782912544e-10, 2.1611563493645347e-10, 1.6922640827705948e-10, 2.5571109335889575e-10, 9.319056529059899e-11, 3.6422843040229136e-10, 2.8611993004891423e-10, 4.26549259021497e-10, 3.202782949875832e-10, 1.0447919005646122e-10, 2.9606143513334626e-10, 1.0213740313583242e-10, 2.8706856054598606e-10, 1.819916543854827e-10, 2.4297645341044005e-10, 5.6871850544898406e-11, 3.1144907811607447e-10, 1.5469699739950604e-10, 2.4428550263362603e-10, 1.2078406417398435e-11, 1.6825040904770716e-10, 8.107177602856286e-11, 4.308205946938065e-10, 2.6508789737964e-10, 3.183712059175864e-10, 4.134428329208295e-10, 4.056190446456026e-10, 2.360341683389777e-10, 1.1747155396861497e-10, 2.4099576501068753e-11, 6.968781649831224e-11, 2.6245765933838316e-10, 3.238477678298851e-10, 4.4052659003285255e-10, 1.7037198496612505e-10, 1.6611228581889237e-10, 2.284650166533425e-10, 3.8451322507959684e-10, 2.2946896141053108e-10, 4.5639745932177234e-10, 4.5205143599756636e-10, 3.3298531380149266e-10, 1.2296394740103633e-10, 1.6047425931489667e-10, 3.35246237671491e-10, 2.6399308086788587e-10, 1.7556054028068646e-10, 4.041166043293465e-10, 3.538290125303101e-11, 6.115206865192624e-11, 4.5394650646964074e-10, 2.827758361668986e-10, 4.4685154426508944e-10, 1.7547106302695514e-10, 1.755653818939079e-10, 2.005063003650276e-10, 3.4841194664590736e-10, 3.7633165073797104e-10, 2.451403518111822e-10, 1.4755291129556203e-10, 1.9254638226710863e-10, 2.4415013066825253e-10, 5.683403595836689e-11, 1.7800640085205433e-10, 2.986915504429172e-10, 1.6780898458995663e-10, 4.218550901397694e-10, 3.3721427256452885e-10, 7.147103633746432e-11, 4.457918947181616e-10, 3.4104951718641474e-11, 3.154338504805787e-10, 4.4126749975358093e-10, 4.8077237503607595e-11, 1.2764336397748377e-10, 2.683670304211294e-10, 3.1577225274685705e-10, 8.830463790743259e-11, 8.195313373239588e-11, 2.870953991034042e-10, 4.15722320934106e-10, 5.909218166871844e-11, 2.5766619090109022e-11, 4.237166160471484e-10, 1.6118848855488366e-10, 2.61922041347451e-10, 1.8035371134907108e-10, 1.5218458567986537e-10, 5.0301847539721556e-11, 3.471172628846664e-10, 4.201790506242614e-10, 1.0197688917260028e-10, 3.1905651102885624e-10, 4.104081588462982e-10, 3.637655996736877e-11, 2.8747913034463113e-10, 4.0783412736354607e-10, 1.495465027853693e-10, 2.496264629465311e-10, 4.646773890150391e-10, 2.6783617576402186e-10, 1.9915953435034606e-10, 1.446571233024857e-10, 9.767744729369254e-11, 1.1682779548459477e-10, 2.1597939495882068e-10, 2.144890543388095e-10, 3.070129710354419e-10, 1.4304041696770742e-10, 1.6094644020303583e-10, 4.613531382011843e-10, 1.599871515649276e-10, 2.626595794162634e-11, 2.81644623421895e-10, 2.5748109308727785e-10, 3.8521959797380156e-10, 2.4065114029130086e-10, 1.8797066160536335e-10, 4.168775604666136e-10, 3.523849627894149e-10, 3.5161181756969417e-10, 4.4073706229746934e-10, 2.4577194381210365e-10, 4.623274547487649e-10, 6.119141304772313e-11, 2.674076916928808e-10, 1.9144740914106562e-11, 9.09130754619264e-11, 3.6607132169325096e-10, 1.2923814557593005e-10, 2.878177162747575e-10, 1.6806753102942273e-10, 1.5427423748215485e-10, 5.988020864745869e-11, 1.736336538958605e-10, 1.079889699797526e-10, 1.0034213824422489e-10, 4.050782907889794e-11, 2.0126508142384592e-10, 2.989692354359724e-10, 4.49517739004035e-10, 4.0142368097108305e-10, 2.8779588044300364e-10, 1.0381291984905283e-10, 9.068517378002516e-11, 3.367974729402007e-10, 1.612593795476125e-10, 3.1950894252390227e-10, 1.940645855819101e-10, 3.7661570411774703e-10, 3.7853345588242937e-10, 1.863003794202317e-10, 1.8655713714295363e-10, 2.6095809237472456e-10, 2.2388684721257046e-10, 5.904193106642808e-11, 6.314524445862058e-11, 9.08108961292231e-12, 5.358259647612884e-11, 2.6464111284318914e-11, 2.8885962133524956e-10, 5.526836844085836e-11, 1.5585817944412106e-10, 2.8237323417001914e-10, 1.9529959962606702e-10, 2.5897168914532087e-10, 4.4737914566515824e-10, 5.6209461564427077e-11, 4.1230780183261484e-10, 3.852933948451931e-10, 4.3601804523667187e-10, 3.8319692211019607e-10, 3.7225983602097446e-10, 2.8068941919021095e-10, 3.572112152513268e-10, 4.051445420469313e-10, 1.3536331886117303e-10, 2.5512759136217533e-10, 1.338947449008171e-10, 4.4789189891647485e-10, 8.454247848663221e-11, 2.2000396795139598e-10, 3.9393898299683316e-10, 4.597901775028196e-10, 4.025328096020353e-10, 2.384483034200552e-10, 2.624195999390011e-10, 1.1923897828315655e-10, 2.3852984821701173e-10, 4.048683677811832e-10, 3.6252175827228283e-10, 2.436165273417973e-10, 5.931820529561649e-11, 2.5110331760939963e-10, 1.6272891020796543e-11, 2.0566198266971925e-11, 3.4584716123928216e-10, 1.053366332961353e-10, 1.3303513932833022e-10, 2.6469378121632414e-11, 4.056011156279571e-10, 3.085095900533935e-10, 2.4972537579492915e-10, 3.6374347479363467e-10, 2.5321990598098365e-11, 3.64341615688088e-10, 4.413793558075141e-10, 2.0876620679573243e-10, 2.824490350807063e-10, 4.53127378284221e-10, 1.1305853649112851e-10, 1.5285812221023376e-10, 6.647212077645626e-11, 5.6069206352468703e-11, 3.5242865310119997e-10, 1.0402147827999475e-10, 4.6104774555424943e-10, 3.112662703540908e-10, 4.57024730560196e-10, 1.8546416334121185e-10, 3.3896503225799846e-11, 1.91506255081339e-10, 4.378057803441915e-10, 1.925745164461229e-10, 1.7025054499866799e-10, 1.0990026141736997e-10, 1.1301386042272821e-10, 3.8776285329368554e-10, 2.372407804061838e-10, 2.49538943111402e-10, 7.46827261491334e-11, 1.2615812843182195e-10, 3.1800848109345137e-10, 2.3829130747603233e-10, 2.6408491018981017e-10, 3.51204553503473e-10, 1.5049549410492413e-10, 3.7778942951413597e-11, 4.1478597466448885e-11, 3.995498664994407e-10, 3.681655453592614e-10, 2.857439309039006e-10, 1.4305600627706472e-10, 2.765442321753936e-10, 2.65546287649826e-10, 9.819805537290838e-11, 1.6590632079954704e-10, 2.3872533051923217e-10, 4.848972308749144e-11, 3.494586015961171e-10, 3.5579805768293227e-10, 4.62705441886882e-10, 3.9505679240089964e-10, 1.9191003625650127e-10, 4.072713373146075e-10, 8.741668261653956e-11, 3.020044513947495e-10, 4.2153230797419006e-10, 3.255546440067425e-10, 2.717753489703961e-10, 1.2619646213435365e-11, 5.405119906341793e-11, 2.5074813145981034e-10, 3.1017320244360047e-10, 2.2199147424808996e-10, 1.8341275438531202e-10, 1.0283647197684154e-10, 2.9542111313653197e-10, 1.6451435656469893e-10, 9.045335639355778e-11, 1.389248915559249e-10, 3.028754688456231e-10, 2.1521341523125792e-10, 1.3912875951142822e-10, 2.163237818042507e-11, 2.7214702041401806e-10, 4.3381398603589694e-10, 1.534639513991326e-10, 2.8400342928086575e-10, 3.3900960445983064e-10, 2.1748517266620415e-10, 3.620065573261416e-10, 6.458202722603446e-11, 2.0597296633041717e-10, 2.70066406304198e-10, 1.2220367751579475e-10, 1.202833692023897e-10, 9.810484824686327e-11, 4.121747340136983e-10, 1.1450101175362881e-10, 3.3409416726193353e-10, 2.992141625614286e-11, 2.7937247567416668e-11, 4.2461643728029796e-10, 2.595757575898916e-10, 3.955924782628878e-10, 2.6068182378338745e-10, 2.037868942605242e-10, 3.011040063116699e-10, 1.4261927945048714e-10, 3.1783068147814286e-10, 1.7300520369599448e-10, 4.30288068196949e-10, 1.9784420044084705e-10, 3.5845970722953036e-11, 1.5686834014920492e-10, 9.109872058099544e-11, 2.4046972963223667e-11, 1.3414932489505538e-10, 3.6363438106999657e-10, 3.8509991463070437e-10, 9.826089876659172e-11, 2.016711212089739e-10, 4.5975480432274007e-10, 8.212079627423208e-11, 5.270883577691843e-11, 2.804512867597697e-10, 3.320519532078181e-11, 5.193917084617139e-11, 1.472182768486735e-10, 3.635066076271287e-11, 8.181648305898015e-11, 2.117330668433004e-10, 4.499721754057384e-10, 2.733653076038983e-10, 3.2524236168868237e-10, 4.177725624211065e-10, 1.552057167682197e-10, 4.611900971795285e-10, 3.899293075926291e-10, 2.9013455028238655e-10, 2.0606038706946772e-10, 2.707688827926352e-10, 3.06547737714824e-10, 1.1617468900944339e-10, 8.597228354570441e-11, 1.1875126815046044e-10, 1.5534523840898817e-10, 3.333952352472386e-10, 1.372353771421364e-10, 2.7384830666131454e-10, 3.076467804466465e-10, 3.594671151452983e-10, 2.096629159349661e-10, 3.897501286553168e-10, 3.6873765846268114e-10, 1.6014194679538074e-10, 3.9290119031518134e-10, 3.104266563854624e-10, 3.717458172888821e-10, 3.756149603590525e-10, 1.5076110715731006e-10, 3.8702966417662743e-10, 4.244736905717472e-10, 5.946432604132801e-11, 3.2500554808176374e-10, 2.441063720517306e-10, 3.6623448110978396e-10, 1.646875025674427e-10, 1.3600637326430243e-10, 2.87939565203274e-10, 2.932558813839581e-10, 1.8942793362543286e-10, 1.6696706820959473e-10, 2.381115545620899e-10, 2.5007715256236684e-10, 3.0786429070124577e-10, 1.9949588378823102e-10, 3.7209083275473176e-10, 4.111638477605206e-10, 1.2414588823096706e-10, 9.32577431077275e-11, 2.2960002800907764e-10, 5.0166097838311163e-11, 3.212497344962789e-10, 2.8070362440707486e-11, 3.4633504397696413e-10, 2.0528799341869297e-10, 3.849309755492303e-10, 2.770903157530563e-10, 1.2297589162268974e-11, 2.1110904452510681e-10, 3.783068324748895e-10, 2.6423232937604346e-10, 4.342362355108653e-10, 4.1414479701532e-11, 3.727887352110437e-10, 3.840196682782654e-10, 4.6110972071883305e-10, 1.0825136208432451e-10, 1.1521471710712605e-11, 4.9538481606756224e-11, 3.2792894441580067e-10, 2.087310968599404e-10, 3.4913942180067603e-10, 2.640944301354059e-10, 4.376593453966904e-10, 3.098665154761121e-10, 2.539760472243524e-10, 4.347411551846453e-10, 5.1264313974397346e-11, 1.6834624688190358e-10, 3.9454969976912557e-10, 4.454166078939753e-10, 2.3691229273509196e-10, 3.3183154076084354e-10, 4.184815866233038e-10, 1.7530876121434058e-10, 2.953783814763078e-10, 3.162720417591164e-10, 3.1054189037968416e-10, 5.677376602696016e-11, 3.945662711488107e-10, 2.3111597813492424e-10, 3.855443416779514e-10, 4.463635214484868e-10, 9.584260989015103e-11, 2.4992740190778184e-10, 1.9257563382488185e-10, 1.4365670957493248e-10, 3.828354655857624e-10, 7.334358164204358e-12, 9.950572353936704e-11, 2.1335343626353864e-10, 2.7237284996872724e-10, 2.3606250483011737e-10, 2.4632995946835545e-10, 4.068114835883291e-10, 2.748258417714644e-10, 3.4307488902714234e-10, 1.7062779531298866e-10, 3.9208105056913467e-10, 5.82419754785457e-11, 2.9216913632369723e-10, 3.1294705151937996e-10, 7.35352974259168e-11, 2.552269966552001e-10, 2.6391471690326296e-11, 3.234953108340044e-10, 3.428267457243617e-11, 4.235735234781046e-10, 1.4100595539280614e-10, 3.4560284755541515e-11, 1.0774865006300816e-10, 3.056269915965859e-10, 3.0000736378661685e-10, 3.483471727218357e-10, 3.7124182786056614e-10, 1.2267772041274494e-10, 3.8238672861803935e-11, 1.2576781760129108e-10, 2.395856341010777e-10, 6.445016829254024e-11, 3.0474672373277667e-10, 2.9570829855954384e-10]\n"
     ]
    }
   ],
   "source": [
    "def problem3_accept_reject(uniformGenerator=None, n_iterations=None, seed=0):\n",
    "    \"\"\"\n",
    "    Takes a generator that produces uniform pseudo random [0,1] numbers\n",
    "    and produces samples from (pi/2)*abs(sin(x*2*pi)) using an Accept-Reject\n",
    "    sampler with the uniform distribution as the proposal distribution.\n",
    "    Runs n_iterations.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    generator : function\n",
    "        A function of the type generator(size, seed) that produces uniform pseudo-random\n",
    "        numbers from [0,1].\n",
    "    seed : int\n",
    "        The seed to be used in the generator provided.\n",
    "    n_iterations : int\n",
    "        An integer denoting how many attempts should be made in the accept-reject sampler.\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    out : list\n",
    "        A list of the pseudo-random numbers with the specified distribution.\n",
    "    \"\"\"\n",
    "    if uniformGenerator is None:\n",
    "        raise ValueError(\"A valid uniform generator function must be provided.\")\n",
    "\n",
    "    import math\n",
    "\n",
    "    # The target density function f(x)\n",
    "    def target_density(x):\n",
    "        return math.pi * abs(math.sin(2 * math.pi * x))\n",
    "\n",
    "    # The proposal density g(x), which is Uniform[0, 1], so g(x) = 1 for x in [0, 1].\n",
    "    # A uniform distribution over the interval [0, 1] assigns equal probability density to every point \n",
    "    # x in [0, 1]. The total area under the PDF must equal 1, as it represents a probability distribution.\n",
    "    def proposal_density(x):\n",
    "        return 1\n",
    "\n",
    "    # Maximum value of f(x)/g(x), i.e., M. In the context of the Accept-Reject algorithm, M represents the upper bound on the ratio of the target density \n",
    "    # f(x) to the proposal density g(x).\n",
    "    M = math.pi  # As f(x) is bounded by pi.\n",
    "\n",
    "    # Generate uniform random numbers using the provided generator\n",
    "    uniform_numbers = uniformGenerator(size=n_iterations, seed=seed)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for u in uniform_numbers:\n",
    "        x = u  # Sample from the proposal density g(x), which is uniform.\n",
    "        r_x = target_density(x) / (M * proposal_density(x))  # Acceptance ratio\n",
    "        u2 = uniformGenerator(size=1, seed=seed + 1)[0]  # Generate a new uniform random number\n",
    "\n",
    "        if u2 <= r_x:\n",
    "            samples.append(x)  # Accept the sample\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Example usage with the uniform generator\n",
    "accepted_samples = problem3_accept_reject(\n",
    "    uniformGenerator=lambda size, seed: problem3_uniform(\n",
    "        generator=problem3_LCG,\n",
    "        period=2**31,  # Match the period of problem3_LCG\n",
    "        size=size,\n",
    "        seed=seed\n",
    "    ),\n",
    "    n_iterations=1000,\n",
    "    seed=42\n",
    ")\n",
    "print(accepted_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCG output: [0.5138700781390071, 0.1757413032464683, 0.3086515162140131, 0.5345338867045939, 0.9476279253140092, 0.17173630138859153, 0.7022311687469482, 0.2264306810684502, 0.49477344658225775, 0.12472031963989139]\n",
      "Uniform sampler [2.392894020951386e-10, 8.183592150289021e-11, 1.437270623697029e-10, 2.4891173779247044e-10, 4.412736395904837e-10, 7.997094718208142e-11, 3.270018700263222e-10, 1.0544000243230267e-10, 2.303968400611811e-10, 5.807742459694454e-11]\n",
      "Accept-Reject sampler [2.392894020951386e-10, 8.183592150289021e-11, 1.437270623697029e-10, 2.4891173779247044e-10, 4.412736395904837e-10, 7.997094718208142e-11, 3.270018700263222e-10, 1.0544000243230267e-10, 2.303968400611811e-10, 5.807742459694454e-11, 3.906842471063099e-11, 1.8144357999579086e-10, 1.2909332153601683e-10, 1.7139628496858073e-10, 4.5794862867098995e-10, 2.493140938922972e-10, 3.5654842056240255e-10, 3.010375193481657e-10, 3.5722684619404754e-10, 3.63326128948252e-10]\n",
      "Accept-Reject sampler [0.763774618976614, 0.2550690257394217, 0.7887233511355132, 0.762280082457942, 0.7215400323407826, 0.22876222127045265]\n"
     ]
    }
   ],
   "source": [
    "#local test \n",
    "# If you managed to solve all three parts you can test the following code to see if it runs\n",
    "# you have to change the period to match your LCG though, this is marked as X.\n",
    "# It is a very good idea to check these things using the histogramfunction in sagemath\n",
    "# try with a larger number of samples, up to 10000 should run\n",
    "print(\"LCG output: %s\" % problem3_LCG(size=10, seed = 1)) \n",
    "period = 2**31\n",
    "print(\"Uniform sampler %s\" % problem3_uniform(generator=problem3_LCG, period = period, size=10, seed=1))\n",
    "uniform_sampler = lambda size,seed: problem3_uniform(generator=problem3_LCG, period = period, size=size, seed=seed)\n",
    "print(\"Accept-Reject sampler %s\" % problem3_accept_reject(uniformGenerator = uniform_sampler,n_iterations=20,seed=1))\n",
    "\n",
    "\n",
    "# If however you did not manage to implement either part 1 or part 2 but still want to check part 3, you can run the code below\n",
    "def testUniformGenerator(size,seed): \n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    return [random.uniform(0,1) for s in range(size)]\n",
    "print(\"Accept-Reject sampler %s\" % problem3_accept_reject(uniformGenerator=testUniformGenerator, n_iterations=20, seed=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Markovian travel\n",
    "\n",
    "The dataset `Travel Dataset - Datathon 2019` is a simulated dataset designed to mimic real corporate travel systems -- focusing on flights and hotels. The file is at `data/flights.csv`, i.e. you can use the path `data/flights.csv` from the notebook to access the file.\n",
    "\n",
    "1. In the first code-box \n",
    "    1. Load the csv from file `data/flights.csv`\n",
    "    2. Fill in the value of the variables as specified by their names.\n",
    "2. In the second code-box your goal is to estimate a Markov chain transition matrix for the travels of these users. For example, if we enumerate the cities according to alphabetical order, the first city `'Aracaju (SE)'` would correspond to $0$. Each row of the file corresponds to one flight, i.e. it has a starting city and an ending city. We model this as a stationary Markov chain, i.e. each user's travel trajectory is a realization of the Markov chain, $X_t$. Here, $X_t$ is the current city the user is at, at step $t$, and $X_{t+1}$ is the city the user travels to at the next time step. This means that to each row in the file there is a corresponding pair $(X_{t},X_{t+1})$. The stationarity assumption gives that for all $t$ there is a transition density $p$ such that $P(X_{t+1} = y | X_t = x) = p(x,y)$ (for all $x,y$). The transition matrix should be `n_cities` x `n_citites` in size.\n",
    "3. Use the transition matrix to compute out the stationary distribution.\n",
    "4. Given that we start in 'Aracaju (SE)' what is the probability that after 3 steps we will be back in 'Aracaju (SE)'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>travelCode</th>\n",
       "      <th>userCode</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>flightType</th>\n",
       "      <th>price</th>\n",
       "      <th>time</th>\n",
       "      <th>distance</th>\n",
       "      <th>agency</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recife (PE)</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1434.38</td>\n",
       "      <td>1.76</td>\n",
       "      <td>676.53</td>\n",
       "      <td>FlyingDrops</td>\n",
       "      <td>09/26/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>Recife (PE)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1292.29</td>\n",
       "      <td>1.76</td>\n",
       "      <td>676.53</td>\n",
       "      <td>FlyingDrops</td>\n",
       "      <td>09/30/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Brasilia (DF)</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1487.52</td>\n",
       "      <td>1.66</td>\n",
       "      <td>637.56</td>\n",
       "      <td>CloudFy</td>\n",
       "      <td>10/03/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>Brasilia (DF)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1127.36</td>\n",
       "      <td>1.66</td>\n",
       "      <td>637.56</td>\n",
       "      <td>CloudFy</td>\n",
       "      <td>10/04/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Aracaju (SE)</td>\n",
       "      <td>Salvador (BH)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1684.05</td>\n",
       "      <td>2.16</td>\n",
       "      <td>830.86</td>\n",
       "      <td>CloudFy</td>\n",
       "      <td>10/10/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   travelCode  userCode                from                  to  flightType  \\\n",
       "0           0         0         Recife (PE)  Florianopolis (SC)  firstClass   \n",
       "1           0         0  Florianopolis (SC)         Recife (PE)  firstClass   \n",
       "2           1         0       Brasilia (DF)  Florianopolis (SC)  firstClass   \n",
       "3           1         0  Florianopolis (SC)       Brasilia (DF)  firstClass   \n",
       "4           2         0        Aracaju (SE)       Salvador (BH)  firstClass   \n",
       "\n",
       "     price  time  distance       agency        date  \n",
       "0  1434.38  1.76    676.53  FlyingDrops  09/26/2019  \n",
       "1  1292.29  1.76    676.53  FlyingDrops  09/30/2019  \n",
       "2  1487.52  1.66    637.56      CloudFy  10/03/2019  \n",
       "3  1127.36  1.66    637.56      CloudFy  10/04/2019  \n",
       "4  1684.05  2.16    830.86      CloudFy  10/10/2019  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/flights.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "1335\n",
      "271888\n"
     ]
    }
   ],
   "source": [
    "number_of_cities = len(set(data['from']).union(set(data['to'])))  # Unique cities from both 'from' and 'to' columns\n",
    "number_of_userCodes = data['userCode'].nunique()  # Unique user codes\n",
    "number_of_observations = data.shape[0]  # Number of rows in the dataset\n",
    "\n",
    "print(number_of_cities)\n",
    "print(number_of_userCodes)\n",
    "print(number_of_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a very useful function that you can use for part 2. You have seen this before when parsing the\n",
    "# pride and prejudice book.\n",
    "\n",
    "def makeFreqDict(myDataList):\n",
    "    '''Make a frequency mapping out of a list of data.'''\n",
    "    freqDict = {} # start with an empty dictionary\n",
    "\n",
    "    for res in myDataList:\n",
    "        if res in freqDict: # the data value already exists as a key\n",
    "                freqDict[res] = freqDict[res] + 1 # add 1 to the count\n",
    "        else: # the data value does not exist as a key value\n",
    "            freqDict[res] = 1 # add a new key-value pair for this new data value, frequency 1\n",
    "\n",
    "    return freqDict  # return the dictionary created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05305717, 0.11258865, 0.14572588, 0.2322695 , 0.09605362,\n",
       "        0.11378412, 0.07546207, 0.07844401, 0.09261498],\n",
       "       [0.13687904, 0.04137561, 0.14682088, 0.25281848, 0.0954222 ,\n",
       "        0.10328471, 0.06434582, 0.06514182, 0.09391143],\n",
       "       [0.15566364, 0.12937435, 0.        , 0.23751007, 0.10135835,\n",
       "        0.13055428, 0.06951479, 0.07252216, 0.10350236],\n",
       "       [0.150803  , 0.13579859, 0.14398995, 0.        , 0.11756649,\n",
       "        0.13263196, 0.10131463, 0.1011925 , 0.11670287],\n",
       "       [0.14992015, 0.12491595, 0.14786099, 0.2832829 , 0.02891242,\n",
       "        0.1209447 , 0.04011178, 0.03899815, 0.06505295],\n",
       "       [0.13720472, 0.10579068, 0.1480479 , 0.249229  , 0.09504593,\n",
       "        0.0425853 , 0.06231955, 0.06414042, 0.09563648],\n",
       "       [0.16886708, 0.11656259, 0.14362177, 0.34534642, 0.05509961,\n",
       "        0.11385668, 0.        , 0.        , 0.05664585],\n",
       "       [0.17095416, 0.11681478, 0.14733396, 0.33910196, 0.05472404,\n",
       "        0.11415458, 0.        , 0.        , 0.05691651],\n",
       "       [0.14689947, 0.12192593, 0.1517672 , 0.28325926, 0.06668783,\n",
       "        0.12205291, 0.04016931, 0.04131217, 0.02592593]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data (replace this with your full dataset)\n",
    "cities = data['from'].tolist() + data['to'].tolist()\n",
    "#['Aracaju (SE)', 'Brasilia (DF)', 'Campo Grande (MS)','Florianopolis (SC)','Natal (RN)','Recife (PE)','Rio de Janeiro (RJ)','Salvador (BH)','Sao Paulo (SP)']\n",
    "# Unique cities sorted alphabetically\n",
    "unique_cities = sorted(set(cities))  # The unique cities\n",
    "n_cities = len(unique_cities)  # The number of unique cities\n",
    "\n",
    "# Count the different transitions\n",
    "transitions = [(cities[i], cities[i + 1]) for i in range(len(cities) - 1)]  # A list of transitions (from city -> to city)\n",
    "\n",
    "# Transition counts: counting occurrences of each transition\n",
    "transition_counts = {}\n",
    "for transition in transitions:\n",
    "    if transition in transition_counts:\n",
    "        transition_counts[transition] += 1\n",
    "    else:\n",
    "        transition_counts[transition] = 1\n",
    "\n",
    "# Mapping cities to indices and vice versa\n",
    "indexToCity = {i: city for i, city in enumerate(unique_cities)}  # Index to City\n",
    "cityToIndex = {city: i for i, city in enumerate(unique_cities)}  # City to Index\n",
    "\n",
    "# Transition matrix initialization\n",
    "transition_matrix = np.zeros((n_cities, n_cities))\n",
    "\n",
    "# Fill the transition matrix with probabilities (count / total transitions from a city)\n",
    "for (from_city, to_city), count in transition_counts.items():\n",
    "    from_idx = cityToIndex[from_city]\n",
    "    to_idx = cityToIndex[to_city]\n",
    "    \n",
    "    # To avoid division by zero, check if there are any transitions from the 'from_city'\n",
    "    total_from_city = sum(1 for t in transitions if t[0] == from_city)\n",
    "    \n",
    "    # Calculate transition probability\n",
    "    if total_from_city > 0:\n",
    "        transition_matrix[from_idx, to_idx] = count / total_from_city\n",
    "    else:\n",
    "        transition_matrix[from_idx, to_idx] = 0  # No transitions, set probability to 0\n",
    "\n",
    "transition_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cities = XXX\n",
    "unique_cities = sorted(set(cities)) # The unique cities\n",
    "n_cities = len(unique_cities) # The number of unique citites\n",
    "\n",
    "# Count the different transitions\n",
    "transitions = XXX # A list containing tuples ex: ('Aracaju (SE)','Rio de Janeiro (RJ)') of all transitions in the text\n",
    "transition_counts = XXX # A dictionary that counts the number of each transition \n",
    "# ex: ('Aracaju (SE)','Rio de Janeiro (RJ)'):4\n",
    "indexToCity = XXX # A dictionary that maps the n-1 number to the n:th unique_city,\n",
    "# ex: 0:'Aracaju (SE)'\n",
    "cityToIndex = XXX # The inverse function of indexToWord, \n",
    "# ex: 'Aracaju (SE)':0\n",
    "\n",
    "# Part 3, finding the maximum likelihood estimate of the transition matrix\n",
    "\n",
    "transition_matrix = XXX # a numpy array of size (n_cities,n_cities)\n",
    "\n",
    "# The transition matrix should be ordered in such a way that\n",
    "# p_{'Aracaju (SE)','Rio de Janeiro (RJ)'} = transition_matrix[cityToIndex['Aracaju (SE)'],cityToIndex['Rio de Janeiro (RJ)']]\n",
    "# and represents the probability of travelling Aracaju (SE)->Rio de Janeiro (RJ)\n",
    "\n",
    "# Make sure that the transition_matrix does not contain np.nan from division by zero for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13690959 0.11320495 0.12780285 0.2108111  0.08752152 0.1121035\n",
      " 0.06184548 0.06290842 0.08689258]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This should be a numpy array of length n_cities which sums to 1 and is all positive\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Assume transition_matrix is already available (from Problem 2)\n",
    "\n",
    "# Step 1: Compute the stationary distribution\n",
    "# Use numpy's eig function to get eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(transition_matrix.T)\n",
    "\n",
    "# The eigenvector corresponding to eigenvalue 1 will be the stationary distribution\n",
    "# Find the eigenvector corresponding to eigenvalue 1\n",
    "stationary_distribution_problem2 = eigenvectors[:, np.isclose(eigenvalues, 1)].flatten()\n",
    "\n",
    "# Normalize the stationary distribution to ensure it sums to 1\n",
    "stationary_distribution_problem2 /= stationary_distribution_problem2.sum()\n",
    "\n",
    "print(stationary_distribution_problem2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of returning to 'Aracaju (SE)' after 3 steps is: 0.13592604045657683\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute the return probability \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Assume transition_matrix is already available (from Problem 2)\n",
    "# Assume 'Aracaju (SE)' corresponds to the index 'i' in cityToIndex\n",
    "\n",
    "# Step 1: Compute the 3-step transition matrix\n",
    "transition_matrix_3_steps = np.linalg.matrix_power(transition_matrix, 3)\n",
    "\n",
    "# Step 2: Extract the probability of transitioning from 'Aracaju (SE)' to 'Aracaju (SE)'\n",
    "# 'Aracaju (SE)' is mapped to index, say i\n",
    "start_city = 'Aracaju (SE)'\n",
    "end_city = 'Aracaju (SE)'\n",
    "\n",
    "# Find the index of 'Aracaju (SE)' in cityToIndex\n",
    "start_idx = cityToIndex[start_city]\n",
    "end_idx = cityToIndex[end_city]\n",
    "\n",
    "# The probability of returning to 'Aracaju (SE)' after 3 steps\n",
    "return_probability_problem2 = transition_matrix_3_steps[start_idx, end_idx]\n",
    "\n",
    "# Output the result\n",
    "print(f\"The probability of returning to '{start_city}' after 3 steps is: {return_probability_problem2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aracaju (SE)->Salvador (BH)->Recife (PE)->Campo Grande (MS)->Florianopolis (SC)->Aracaju (SE)->Campo Grande (MS)->Brasilia (DF)->Florianopolis (SC)->Salvador (BH)->"
     ]
    }
   ],
   "source": [
    "# Local test\n",
    "# Once you have created all your functions, you can make a small test here to see\n",
    "# what would be generated from your model.\n",
    "import numpy as np\n",
    "\n",
    "start = np.zeros(shape=(n_cities,1))\n",
    "start[cityToIndex['Aracaju (SE)'],0] = 1\n",
    "\n",
    "current_pos = start\n",
    "for i in range(10):\n",
    "    random_word_index = np.random.choice(range(n_cities),p=current_pos.reshape(-1))\n",
    "    current_pos = np.zeros_like(start)\n",
    "    current_pos[random_word_index] = 1\n",
    "    print(indexToCity[random_word_index],end='->')\n",
    "    current_pos = (current_pos.T@transition_matrix).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use the **Multi-dimensional Constrained Optimisation** example (in `07-Optimization.ipynb`) to numerically find the MLe for the mean and variance parameter based on `normallySimulatedDataSamples`, an array obtained by a specific simulation of $30$ IID samples from the $Normal(10,2)$ random variable.\n",
    "\n",
    "Recall that $Normal(\\mu, \\sigma^2)$ RV has the probability density function given by:\n",
    "\n",
    "$$\n",
    "f(x ;\\mu, \\sigma) = \\displaystyle\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(\\frac{-1}{2\\sigma^2}(x-\\mu)^2\\right)\n",
    "$$\n",
    "\n",
    "The two parameters, $\\mu \\in \\mathbb{R} := (-\\infty,\\infty)$ and $\\sigma \\in (0,\\infty)$, are sometimes referred to as the location and scale parameters.\n",
    "\n",
    "You know that the log likelihood function for $n$ IID samples from a Normal RV with parameters $\\mu$ and $\\sigma$ simply follows from $\\sum_{i=1}^n \\log(f(x_i; \\mu,\\sigma))$, based on the IID assumption. \n",
    "\n",
    "NOTE: When setting bounding boxes for $\\mu$ and $\\sigma$ try to start with some guesses like $[-20,20]$ and $[0.1,5.0]$ and make it larger if the solution is at the boundary. Making the left bounding-point for $\\sigma$ too close to $0.0$ will cause division by zero Warnings. Other numerical instabilities can happen in such iterative numerical solutions to the MLe. You need to be patient and learn by trial-and-error. You will see the mathematical theory in more details in a future course in scientific computing/optimisation. So don't worry too much now except learning to use it for our problems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative log-likelihood function for a normal distribution is given by:\n",
    "\n",
    "$$\n",
    "-\\log L(\\mu, \\sigma) = n \\log(\\sigma) + \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 + \\frac{n}{2} \\log(2\\pi)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 58.63138728247736\n",
       "        x: [ 9.269e+00  1.708e+00]\n",
       "      nit: 5\n",
       "      jac: [ 9.948e-06 -6.537e-05]\n",
       "     nfev: 27\n",
       "     njev: 9\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "\n",
    "# Simulate 30 IID samples drawn from Normal(10,2)RV\n",
    "normallySimulatedDataSamples = np.random.normal(10, 2, 30)\n",
    "\n",
    "# Define the negative log-likelihood function\n",
    "def negLogLklOfIIDNormalSamples(parameters):\n",
    "    '''Return the -log(likelihood) of normallySimulatedDataSamples with mean and variance parameters'''\n",
    "    mu_param = parameters[0]\n",
    "    sigma_param = parameters[1]\n",
    "    \n",
    "    # Calculate the sum of squared differences from the mean\n",
    "    n = len(normallySimulatedDataSamples)\n",
    "    sum_of_squares = np.sum((normallySimulatedDataSamples - mu_param)**2)\n",
    "    \n",
    "    # The negative log-likelihood function for the Normal distribution\n",
    "    neg_log_likelihood = n * np.log(sigma_param) + (sum_of_squares / (2 * sigma_param**2)) + (n / 2) * np.log(2 * np.pi)\n",
    "    \n",
    "    return neg_log_likelihood\n",
    "\n",
    "# Set parameter bounds for mu and sigma\n",
    "parameter_bounding_box = ((-20, 20), (0.1, 5.0))  # Bounds for mu and sigma\n",
    "initial_arguments = np.array([10, 2])  # Initial guess for mu and sigma (mean and std from simulation)\n",
    "\n",
    "# Minimize the negative log-likelihood function\n",
    "result_Ass2Prob4 = optimize.minimize(negLogLklOfIIDNormalSamples, initial_arguments, bounds=parameter_bounding_box)\n",
    "\n",
    "result_Ass2Prob4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
