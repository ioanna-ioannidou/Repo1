{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data X and y , in the cell below. X denotes 20 points in R2 and y corresponds to the labels for these points, i.e. it is a classification problem.\n",
    "1. Implement the function perceptron by filling in XXX .\n",
    "2. Use your implemented perceptron function to compute a vector (numpy array) ùë§ with shape\n",
    "(3,1) such that\n",
    "ÃÇÃÇ\n",
    "(ùë§‚ãÖùë•ùëñ)ùëô_ùëñ >0, ‚àÄùëñ=1,...,20\n",
    "put your answer in hat_w below (the last dimension is the bias dimension, i.e. the added dimension we used to derive the perceptron)\n",
    "ÃÇ\n",
    "3. Use the vector ùë§ that you just found and compute ùëü = maxùëñ |ùë•ùëñ | (put your result in r ), finally\n",
    "use this to give an upper bound to the number of iterations needed for the perceptron algorithm to converge on this dataset, see chapter 8 in the ITDS notes. Put the result in iteration_bound ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0.14774693918368506, 0.8537253157278155],\n",
    "    [-0.1755517430286779, 0.8979710703337818],\n",
    "    [0.5227216475286975, 0.7448281947022451],\n",
    "    [-0.5071170511153492, 0.8002027400836075],\n",
    "    [-0.39436968212400453, 1.0177689414422981],\n",
    "    [-0.3983065780966649, 1.0443663197782966],\n",
    "    [-0.08652771617599643, 0.48036820824519255],\n",
    "    [0.15352541170101042, 0.6820807981911706],\n",
    "    [-0.3303348532791869, 1.120673883903539],\n",
    "    [-0.2656220857139274, 0.8526638282828739],\n",
    "    [0.7259603693529442, 0.25428467532034965],\n",
    "    [0.4577253912481767, -0.2358809079980879],\n",
    "    [0.9722462145222105, 0.13128550836973255],\n",
    "    [0.4089349951770505, -0.09503914544452634],\n",
    "    [0.9718156747909192, 0.3524307824261209],\n",
    "    [1.2009353774940565, -0.25004126389987974],\n",
    "    [1.271791635779178, -0.07571928320750206],\n",
    "    [0.36784476124502913, -0.23743021661715671],\n",
    "    [0.8918396050420891, -0.1029336332277948],\n",
    "    [0.4501578013678095, -0.13188266835015783]\n",
    "]) + np.array([10, 0]).reshape(1, -1)\n",
    "\n",
    "y = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, \n",
    "              -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1\n",
    "\n",
    "def perceptron(X_in, labels, max_iter=1000):\n",
    "    '''\n",
    "    Runs the perceptron algorithm on X_in, labels, and does a maximum of max_iter iterations.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_in: A numpy array of shape (n_samples, n_features), the input data points.\n",
    "    - labels: A numpy array of shape (n_samples,), the labels corresponding to X_in.\n",
    "    - max_iter: The maximum number of iterations to run the algorithm.\n",
    "    \n",
    "    Returns:\n",
    "    - w: A numpy array of shape (n_features + 1,), the learned weight vector including the bias term.\n",
    "    '''\n",
    "    # Add an extra dimension for the bias by appending 1 to each input vector\n",
    "    X = np.hstack((X_in, np.ones((X_in.shape[0], 1))))  # Shape: (n_samples, n_features + 1)\n",
    "    \n",
    "    # Initialize weights as zeros\n",
    "    w = np.zeros(X.shape[1])  # Shape: (n_features + 1,)\n",
    "    \n",
    "    # Run the perceptron algorithm\n",
    "    for _ in range(max_iter):\n",
    "        # Flag to check if we need to continue updating weights\n",
    "        updates_made = False\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Predict using the current weights\n",
    "            prediction = np.sign(np.dot(w, X[i]))\n",
    "            \n",
    "            # If the prediction is incorrect, update the weights\n",
    "            if prediction != labels[i]:\n",
    "                w += labels[i] * X[i]  # Update rule\n",
    "                updates_made = True\n",
    "        \n",
    "        # Stop if no updates are made in this iteration\n",
    "        if not updates_made:\n",
    "            break\n",
    "    \n",
    "    return w  # Shape: (n_features + 1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed hat_w: [[-1.49468903]\n",
      " [34.08611021]\n",
      " [ 3.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Part2\n",
    "# Use the perceptron function to compute w\n",
    "hat_w = perceptron(X, y, max_iter=1000)\n",
    "\n",
    "# Reshape hat_w to ensure it's a column vector of shape (3, 1)\n",
    "hat_w = hat_w.reshape(-1, 1)\n",
    "\n",
    "print(\"Computed hat_w:\", hat_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 11.272045958488231\n",
      "iteration_bound = 77540.03325843245\n"
     ]
    }
   ],
   "source": [
    "#Part3\n",
    "# Compute r as the maximum L2 norm of the feature vectors in X\n",
    "r = np.max(np.linalg.norm(X, axis=1))  # L2 norm along each row\n",
    "\n",
    "# Compute gamma (the margin)\n",
    "gamma = np.min(y * (X @ hat_w[:-1].flatten() + hat_w[-1]))\n",
    "\n",
    "# Compute the norm of the weight vector hat_w\n",
    "w_norm = np.linalg.norm(hat_w)\n",
    "\n",
    "# Compute the iteration bound\n",
    "iteration_bound = (r**2 * w_norm**2) / (gamma**2)\n",
    "\n",
    "print(\"r =\", r)\n",
    "print(\"iteration_bound =\", iteration_bound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem you will need the pandas package and the sklearn package. If you download the updated data folder from the course website you will find a file called indoor_train.csv , this file includes a bunch of positions in (X,Y,Z) and also a location number. The idea is to assign a room number (Location) to the coordinates (X,Y,Z).\n",
    "1. Take the data in the file indoor_train.csv and load it using pandas into a dataframe df_train\n",
    "2. From this dataframe df_train , create two numpy arrays, one Xtrain and Ytrain , they should have sizes (1154,3) and (1154,) respectively. Their dtype should be float64 and\n",
    "int64 respectively.\n",
    "3. Train a Support Vector Classifier, sklearn.svc.SVC , on Xtrain, Ytrain with\n",
    "kernel='linear' and name the trained model svc_train .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv('data/indoor_train.csv')\n",
    "data.head()\n",
    "data.columns\n",
    "df_train = data\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1154, 3)\n",
      "(1154,)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = data[[\" Position Y\", \"Position X\", \"Position Z\"]].to_numpy(dtype=np.float64)\n",
    "Ytrain = data[[\"Location\"]].to_numpy(dtype=np.int64)\n",
    "Ytrain=Ytrain.reshape(-1)\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "svc_train = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "svc_train.fit(Xtrain, Ytrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following problem we will explore SMS spam texts. The dataset is the SMS Spam Collection Dataset and we have provided for you a way to load the data. If you run the appropriate cell below, the result will be in the spam_no_spam variable. The result is a list of tuples with the first position in the tuple being the SMS text and the second being a flag 0 = not spam and 1 = spam .\n",
    "1. Let ùëã be the random variable that represents each SMS text (an entry in the list), and let ùëå represent whether text is spam or not i.e. ùëå ‚àà {0, 1}. Thus P(ùëå = 1) is the probability that we get a spam. The goal is to estimate:\n",
    "P(ùëå = 1|\"free\" or \"prize\" is in ùëã) .\n",
    "That is, the probability that the SMS is spam given that \"free\" or \"prize\" occurs in the SMS. (This is\n",
    "precision). Hint: it is good to remove the upper/lower case of words so that we can also find \"Free\"\n",
    "and \"Prize\"; this can be done with text.lower() if text a string.\n",
    "2. Estimate the probability that the word \"free\" or \"prize\" is in the text given that it is spam. (This is\n",
    "recall) I.e. estimate\n",
    "P(\"free\" or \"prize\" is in ùëã ‚à£ ùëå = 1) .\n",
    "3. Provide a \"90\\%\" interval of confidence around the true probability from part 1. I.e. use the\n",
    "Hoeffding inequality to obtain for your estimate ùëÉÃÇ. Find ùëô > 0 such that the following holds: P(ùëÉÃÇ‚àíùëô‚â§ùîº[ùëÉÃÇ]‚â§ùëÉÃÇ+ùëô)‚â•0.9 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  0),\n",
       " ('Ok lar... Joking wif u oni...', 0),\n",
       " (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "  1),\n",
       " ('U dun say so early hor... U c already then say...', 0),\n",
       " (\"Nah I don't think he goes to usf, he lives around here though\", 0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to get the SMS text data def load_sms():\n",
    "def load_sms():\n",
    "    import csv\n",
    "    lines = []\n",
    "    hamspam = {'ham': 0, 'spam': 1}\n",
    "    with open('data/spam.csv', mode='r',encoding='latin-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        lines = [(line[1],hamspam[line[0]]) for line in reader]\n",
    "    return lines \n",
    "spam_no_spam = load_sms()\n",
    "spam_no_spam[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808695652173913\n"
     ]
    }
   ],
   "source": [
    "# fill in the estimate for part 1 here (should be a number between 0 and 1)\n",
    "def estimate_probability(spam_no_spam):\n",
    "    # Define the words we are looking for\n",
    "    keywords = ['free', 'prize']\n",
    "    \n",
    "    # Count the occurrences for spam and \"free\" or \"prize\" in X\n",
    "    count_free_or_prize_and_spam = 0\n",
    "    count_free_or_prize = 0\n",
    "    total_sms = len(spam_no_spam)\n",
    "    \n",
    "    # Iterate through each SMS text and its label (0 = not spam, 1 = spam)\n",
    "    for text, label in spam_no_spam:\n",
    "        # Convert text to lowercase to ignore case\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check if the text contains 'free' or 'prize'\n",
    "        if 'free' in text_lower or 'prize' in text_lower:\n",
    "            count_free_or_prize += 1\n",
    "            \n",
    "            # If it's also spam, count it as 'free or prize and spam'\n",
    "            if label == 1:\n",
    "                count_free_or_prize_and_spam += 1\n",
    "    \n",
    "    # Calculate the probabilities\n",
    "    P_free_or_prize_and_spam = count_free_or_prize_and_spam / total_sms\n",
    "    P_free_or_prize = count_free_or_prize / total_sms\n",
    "    \n",
    "    # Calculate the conditional probability P(Y=1 | \"free\" or \"prize\" in X)\n",
    "    if P_free_or_prize > 0:\n",
    "        P_Y_given_free_or_prize = P_free_or_prize_and_spam / P_free_or_prize\n",
    "        return P_Y_given_free_or_prize\n",
    "    else:\n",
    "        return 0  # Return 0 if no \"free\" or \"prize\" words are found in any SMS\n",
    "\n",
    "# Run the estimation\n",
    "problem4_hatP = estimate_probability(spam_no_spam)\n",
    "print(problem4_hatP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37349397590361444\n"
     ]
    }
   ],
   "source": [
    "# fill in the estimate for part 2 here (should be a number between 0 and 1)\n",
    "def estimate_recall_probability(spam_no_spam):\n",
    "    # Define the words we are looking for\n",
    "    keywords = ['free', 'prize']\n",
    "    \n",
    "    # Count the occurrences for spam and \"free\" or \"prize\" in X\n",
    "    count_free_or_prize_and_spam = 0\n",
    "    count_spam = 0\n",
    "    total_sms = len(spam_no_spam)\n",
    "    \n",
    "    # Iterate through each SMS text and its label (0 = not spam, 1 = spam)\n",
    "    for text, label in spam_no_spam:\n",
    "        # Convert text to lowercase to ignore case\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # If the SMS is spam, increment spam count\n",
    "        if label == 1:\n",
    "            count_spam += 1\n",
    "            \n",
    "            # Check if the text contains 'free' or 'prize'\n",
    "            if 'free' in text_lower or 'prize' in text_lower:\n",
    "                count_free_or_prize_and_spam += 1\n",
    "    \n",
    "    # Calculate the probabilities\n",
    "    P_free_or_prize_and_spam = count_free_or_prize_and_spam / total_sms\n",
    "    P_spam = count_spam / total_sms\n",
    "    \n",
    "    # Calculate the conditional probability P(\"free\" or \"prize\" in X | Y=1)\n",
    "    if P_spam > 0:\n",
    "        P_free_or_prize_given_spam = P_free_or_prize_and_spam / P_spam\n",
    "        return P_free_or_prize_given_spam\n",
    "    else:\n",
    "        return 0  # Return 0 if there are no spam messages in the dataset\n",
    "\n",
    "# Run the estimation\n",
    "problem4_hatP2 = estimate_recall_probability(spam_no_spam)\n",
    "print(problem4_hatP2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01639573715443197\n"
     ]
    }
   ],
   "source": [
    "# fill in the calculated l from part 3 here \n",
    "import math\n",
    "\n",
    "def calculate_hoeffding_margin(spam_no_spam, estimate_P):\n",
    "    # Number of SMS samples\n",
    "    n = len(spam_no_spam)\n",
    "    \n",
    "    # Confidence level and delta (error term)\n",
    "    delta = 0.1  # for a 90% confidence level\n",
    "    \n",
    "    # Hoeffding margin of error\n",
    "    l = math.sqrt((1 / (2 * n)) * math.log(2 / delta))\n",
    "    \n",
    "    # Return the margin l\n",
    "    return l\n",
    "\n",
    "# Assuming problem4_hatP has the estimated probability from part 1\n",
    "problem4_l = calculate_hoeffding_margin(spam_no_spam, problem4_hatP)\n",
    "print(problem4_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Number of Samples**: The total number of SMS messages, \\( n \\), is the length of the dataset.\n",
    "2. **Margin of Error \\( l \\)**: We use the Hoeffding inequality formula to compute the margin of error based on the number of samples and the desired confidence level (90% corresponds to \\( \\delta = 0.1 \\)).\n",
    "3. **Confidence Interval**: With the margin \\( l \\), you can compute the confidence interval around the estimate \\( \\hat{P} \\).\n",
    "\n",
    "- `problem4_l` will give you the value of \\( l \\), the margin of error for the confidence interval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
