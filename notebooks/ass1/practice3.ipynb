{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data X and y , in the cell below. X denotes 20 points in R2 and y corresponds to the labels for these points, i.e. it is a classification problem.\n",
    "1. Implement the function perceptron by filling in XXX .\n",
    "2. Use your implemented perceptron function to compute a vector (numpy array) ð‘¤ with shape\n",
    "(3,1) such that\n",
    "Ì‚Ì‚\n",
    "(ð‘¤â‹…ð‘¥ð‘–)ð‘™_ð‘– >0, âˆ€ð‘–=1,...,20\n",
    "put your answer in hat_w below (the last dimension is the bias dimension, i.e. the added dimension we used to derive the perceptron)\n",
    "Ì‚\n",
    "3. Use the vector ð‘¤ that you just found and compute ð‘Ÿ = maxð‘– |ð‘¥ð‘– | (put your result in r ), finally\n",
    "use this to give an upper bound to the number of iterations needed for the perceptron algorithm to converge on this dataset, see chapter 8 in the ITDS notes. Put the result in iteration_bound ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0.14774693918368506, 0.8537253157278155],\n",
    "    [-0.1755517430286779, 0.8979710703337818],\n",
    "    [0.5227216475286975, 0.7448281947022451],\n",
    "    [-0.5071170511153492, 0.8002027400836075],\n",
    "    [-0.39436968212400453, 1.0177689414422981],\n",
    "    [-0.3983065780966649, 1.0443663197782966],\n",
    "    [-0.08652771617599643, 0.48036820824519255],\n",
    "    [0.15352541170101042, 0.6820807981911706],\n",
    "    [-0.3303348532791869, 1.120673883903539],\n",
    "    [-0.2656220857139274, 0.8526638282828739],\n",
    "    [0.7259603693529442, 0.25428467532034965],\n",
    "    [0.4577253912481767, -0.2358809079980879],\n",
    "    [0.9722462145222105, 0.13128550836973255],\n",
    "    [0.4089349951770505, -0.09503914544452634],\n",
    "    [0.9718156747909192, 0.3524307824261209],\n",
    "    [1.2009353774940565, -0.25004126389987974],\n",
    "    [1.271791635779178, -0.07571928320750206],\n",
    "    [0.36784476124502913, -0.23743021661715671],\n",
    "    [0.8918396050420891, -0.1029336332277948],\n",
    "    [0.4501578013678095, -0.13188266835015783]\n",
    "]) + np.array([10, 0]).reshape(1, -1)\n",
    "\n",
    "y = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, \n",
    "              -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1\n",
    "\n",
    "def perceptron(X_in, labels, max_iter=1000):\n",
    "    '''\n",
    "    Runs the perceptron algorithm on X_in, labels, and does a maximum of max_iter iterations.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_in: A numpy array of shape (n_samples, n_features), the input data points.\n",
    "    - labels: A numpy array of shape (n_samples,), the labels corresponding to X_in.\n",
    "    - max_iter: The maximum number of iterations to run the algorithm.\n",
    "    \n",
    "    Returns:\n",
    "    - w: A numpy array of shape (n_features + 1,), the learned weight vector including the bias term.\n",
    "    '''\n",
    "    # Add an extra dimension for the bias by appending 1 to each input vector\n",
    "    X = np.hstack((X_in, np.ones((X_in.shape[0], 1))))  # Shape: (n_samples, n_features + 1)\n",
    "    \n",
    "    # Initialize weights as zeros\n",
    "    w = np.zeros(X.shape[1])  # Shape: (n_features + 1,)\n",
    "    \n",
    "    # Run the perceptron algorithm\n",
    "    for _ in range(max_iter):\n",
    "        # Flag to check if we need to continue updating weights\n",
    "        updates_made = False\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Predict using the current weights\n",
    "            prediction = np.sign(np.dot(w, X[i]))\n",
    "            \n",
    "            # If the prediction is incorrect, update the weights\n",
    "            if prediction != labels[i]:\n",
    "                w += labels[i] * X[i]  # Update rule\n",
    "                updates_made = True\n",
    "        \n",
    "        # Stop if no updates are made in this iteration\n",
    "        if not updates_made:\n",
    "            break\n",
    "    \n",
    "    return w  # Shape: (n_features + 1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed hat_w: [[-1.49468903]\n",
      " [34.08611021]\n",
      " [ 3.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Part2\n",
    "# Use the perceptron function to compute w\n",
    "hat_w = perceptron(X, y, max_iter=1000)\n",
    "\n",
    "# Reshape hat_w to ensure it's a column vector of shape (3, 1)\n",
    "hat_w = hat_w.reshape(-1, 1)\n",
    "\n",
    "print(\"Computed hat_w:\", hat_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 11.272045958488231\n",
      "iteration_bound = 77540.03325843245\n"
     ]
    }
   ],
   "source": [
    "#Part3\n",
    "# Compute r as the maximum L2 norm of the feature vectors in X\n",
    "r = np.max(np.linalg.norm(X, axis=1))  # L2 norm along each row\n",
    "\n",
    "# Compute gamma (the margin)\n",
    "gamma = np.min(y * (X @ hat_w[:-1].flatten() + hat_w[-1]))\n",
    "\n",
    "# Compute the norm of the weight vector hat_w\n",
    "w_norm = np.linalg.norm(hat_w)\n",
    "\n",
    "# Compute the iteration bound\n",
    "iteration_bound = (r**2 * w_norm**2) / (gamma**2)\n",
    "\n",
    "print(\"r =\", r)\n",
    "print(\"iteration_bound =\", iteration_bound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem you will need the pandas package and the sklearn package. If you download the updated data folder from the course website you will find a file called indoor_train.csv , this file includes a bunch of positions in (X,Y,Z) and also a location number. The idea is to assign a room number (Location) to the coordinates (X,Y,Z).\n",
    "1. Take the data in the file indoor_train.csv and load it using pandas into a dataframe df_train\n",
    "2. From this dataframe df_train , create two numpy arrays, one Xtrain and Ytrain , they should have sizes (1154,3) and (1154,) respectively. Their dtype should be float64 and\n",
    "int64 respectively.\n",
    "3. Train a Support Vector Classifier, sklearn.svc.SVC , on Xtrain, Ytrain with\n",
    "kernel='linear' and name the trained model svc_train .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv('data/indoor_train.csv')\n",
    "data.head()\n",
    "data.columns\n",
    "df_train = data\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1154, 3)\n",
      "(1154,)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = data[[\" Position Y\", \"Position X\", \"Position Z\"]].to_numpy(dtype=np.float64)\n",
    "Ytrain = data[[\"Location\"]].to_numpy(dtype=np.int64)\n",
    "Ytrain=Ytrain.reshape(-1)\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "svc_train = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "svc_train.fit(Xtrain, Ytrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following problem we will explore SMS spam texts. The dataset is the SMS Spam Collection Dataset and we have provided for you a way to load the data. If you run the appropriate cell below, the result will be in the spam_no_spam variable. The result is a list of tuples with the first position in the tuple being the SMS text and the second being a flag 0 = not spam and 1 = spam .\n",
    "1. Let ð‘‹ be the random variable that represents each SMS text (an entry in the list), and let ð‘Œ represent whether text is spam or not i.e. ð‘Œ âˆˆ {0, 1}. Thus P(ð‘Œ = 1) is the probability that we get a spam. The goal is to estimate:\n",
    "P(ð‘Œ = 1|\"free\" or \"prize\" is in ð‘‹) .\n",
    "That is, the probability that the SMS is spam given that \"free\" or \"prize\" occurs in the SMS. (This is\n",
    "precision). Hint: it is good to remove the upper/lower case of words so that we can also find \"Free\"\n",
    "and \"Prize\"; this can be done with text.lower() if text a string.\n",
    "2. Estimate the probability that the word \"free\" or \"prize\" is in the text given that it is spam. (This is\n",
    "recall) I.e. estimate\n",
    "P(\"free\" or \"prize\" is in ð‘‹ âˆ£ ð‘Œ = 1) .\n",
    "3. Provide a \"90\\%\" interval of confidence around the true probability from part 1. I.e. use the\n",
    "Hoeffding inequality to obtain for your estimate ð‘ƒÌ‚. Find ð‘™ > 0 such that the following holds: P(ð‘ƒÌ‚âˆ’ð‘™â‰¤ð”¼[ð‘ƒÌ‚]â‰¤ð‘ƒÌ‚+ð‘™)â‰¥0.9 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  0),\n",
       " ('Ok lar... Joking wif u oni...', 0),\n",
       " (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "  1),\n",
       " ('U dun say so early hor... U c already then say...', 0),\n",
       " (\"Nah I don't think he goes to usf, he lives around here though\", 0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to get the SMS text data def load_sms():\n",
    "def load_sms():\n",
    "    import csv\n",
    "    lines = []\n",
    "    hamspam = {'ham': 0, 'spam': 1}\n",
    "    with open('data/spam.csv', mode='r',encoding='latin-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        lines = [(line[1],hamspam[line[0]]) for line in reader]\n",
    "    return lines \n",
    "spam_no_spam = load_sms()\n",
    "spam_no_spam[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808695652173913\n"
     ]
    }
   ],
   "source": [
    "# fill in the estimate for part 1 here (should be a number between 0 and 1)\n",
    "def estimate_probability(spam_no_spam):\n",
    "    # Define the words we are looking for\n",
    "    keywords = ['free', 'prize']\n",
    "    \n",
    "    # Count the occurrences for spam and \"free\" or \"prize\" in X\n",
    "    count_free_or_prize_and_spam = 0\n",
    "    count_free_or_prize = 0\n",
    "    total_sms = len(spam_no_spam)\n",
    "    \n",
    "    # Iterate through each SMS text and its label (0 = not spam, 1 = spam)\n",
    "    for text, label in spam_no_spam:\n",
    "        # Convert text to lowercase to ignore case\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check if the text contains 'free' or 'prize'\n",
    "        if 'free' in text_lower or 'prize' in text_lower:\n",
    "            count_free_or_prize += 1\n",
    "            \n",
    "            # If it's also spam, count it as 'free or prize and spam'\n",
    "            if label == 1:\n",
    "                count_free_or_prize_and_spam += 1\n",
    "    \n",
    "    # Calculate the probabilities\n",
    "    P_free_or_prize_and_spam = count_free_or_prize_and_spam / total_sms\n",
    "    P_free_or_prize = count_free_or_prize / total_sms\n",
    "    \n",
    "    # Calculate the conditional probability P(Y=1 | \"free\" or \"prize\" in X)\n",
    "    if P_free_or_prize > 0:\n",
    "        P_Y_given_free_or_prize = P_free_or_prize_and_spam / P_free_or_prize\n",
    "        return P_Y_given_free_or_prize\n",
    "    else:\n",
    "        return 0  # Return 0 if no \"free\" or \"prize\" words are found in any SMS\n",
    "\n",
    "# Run the estimation\n",
    "problem4_hatP = estimate_probability(spam_no_spam)\n",
    "print(problem4_hatP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37349397590361444\n"
     ]
    }
   ],
   "source": [
    "# fill in the estimate for part 2 here (should be a number between 0 and 1)\n",
    "def estimate_recall_probability(spam_no_spam):\n",
    "    # Define the words we are looking for\n",
    "    keywords = ['free', 'prize']\n",
    "    \n",
    "    # Count the occurrences for spam and \"free\" or \"prize\" in X\n",
    "    count_free_or_prize_and_spam = 0\n",
    "    count_spam = 0\n",
    "    total_sms = len(spam_no_spam)\n",
    "    \n",
    "    # Iterate through each SMS text and its label (0 = not spam, 1 = spam)\n",
    "    for text, label in spam_no_spam:\n",
    "        # Convert text to lowercase to ignore case\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # If the SMS is spam, increment spam count\n",
    "        if label == 1:\n",
    "            count_spam += 1\n",
    "            \n",
    "            # Check if the text contains 'free' or 'prize'\n",
    "            if 'free' in text_lower or 'prize' in text_lower:\n",
    "                count_free_or_prize_and_spam += 1\n",
    "    \n",
    "    # Calculate the probabilities\n",
    "    P_free_or_prize_and_spam = count_free_or_prize_and_spam / total_sms\n",
    "    P_spam = count_spam / total_sms\n",
    "    \n",
    "    # Calculate the conditional probability P(\"free\" or \"prize\" in X | Y=1)\n",
    "    if P_spam > 0:\n",
    "        P_free_or_prize_given_spam = P_free_or_prize_and_spam / P_spam\n",
    "        return P_free_or_prize_given_spam\n",
    "    else:\n",
    "        return 0  # Return 0 if there are no spam messages in the dataset\n",
    "\n",
    "# Run the estimation\n",
    "problem4_hatP2 = estimate_recall_probability(spam_no_spam)\n",
    "print(problem4_hatP2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01639573715443197\n"
     ]
    }
   ],
   "source": [
    "# fill in the calculated l from part 3 here \n",
    "import math\n",
    "\n",
    "def calculate_hoeffding_margin(spam_no_spam, estimate_P):\n",
    "    # Number of SMS samples\n",
    "    n = len(spam_no_spam)\n",
    "    \n",
    "    # Confidence level and delta (error term)\n",
    "    delta = 0.1  # for a 90% confidence level\n",
    "    \n",
    "    # Hoeffding margin of error\n",
    "    l = math.sqrt((1 / (2 * n)) * math.log(2 / delta))\n",
    "    \n",
    "    # Return the margin l\n",
    "    return l\n",
    "\n",
    "# Assuming problem4_hatP has the estimated probability from part 1\n",
    "problem4_l = calculate_hoeffding_margin(spam_no_spam, problem4_hatP)\n",
    "print(problem4_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Number of Samples**: The total number of SMS messages, \\( n \\), is the length of the dataset.\n",
    "2. **Margin of Error \\( l \\)**: We use the Hoeffding inequality formula to compute the margin of error based on the number of samples and the desired confidence level (90% corresponds to \\( \\delta = 0.1 \\)).\n",
    "3. **Confidence Interval**: With the margin \\( l \\), you can compute the confidence interval around the estimate \\( \\hat{P} \\).\n",
    "\n",
    "- `problem4_l` will give you the value of \\( l \\), the margin of error for the confidence interval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
