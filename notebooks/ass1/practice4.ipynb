{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the dataset mammography.mat that you can download from http://odds.cs.stonybrook.edu/mammography-dataset/ (http://odds.cs.stonybrook.edu/mammography- dataset/). Below you can find the code to load this file into X and Y , you just need to put the file in your data folder. During mammography the doctor would be looking for something called\n",
    "calcification , which is calcium deposits in the breast tissue and is used as an indicator of cancer. In this dataset the X corresponds to some measurements, there are 6 features. The Y is a 0-1 label where 1 represents calcification and 0 does not.\n",
    "1. Split the data into three parts, train/test/validation where train is 60% of the data, test is 15% and validation is 25% of the data. Do not do this randomly, I have prepared a shuffling with a fixed seed, this way I can make sure that we all did the same splits. That is [train,test,validation] is the splitting layout.\n",
    "2. Train a machine learning model on the training data (you are free to choose it yourself). Hint: you could always try LogisticRegression , but for it to work well you would need\n",
    "StandardScaler and put everything in a Pipeline .\n",
    "3. Use the classification report from Utils and compute the intervals for precision-recall etc on\n",
    "the test set with union_bound correction with 95% confidence.\n",
    "4. You are interested in minimizing the average cost of your classifier, but first we must define it:\n",
    "If someone is calcified but classified as not, we say it costs 30 (this is the worst scenario) If someone is not calcified but classified as calcified, we say it costs 5 (this probably only costs extra investigation)\n",
    "If someone is calcified but classified as calcified, costs 0 (We did the right thing, no cost) If someone is not calcified but classified as not, costs 0 (We did the right thing, no cost).\n",
    "complete filling the function cost to compute the cost of a prediction model under a certain prediction threshold (recall our precision recall lecture and the predict_proba function from trained models). What would be the cost of having a model that always classifies someone as not calcified on the test set?\n",
    "1. Now, we wish to select the threshold of our classifier that minimizes the cost, we do that by checking say 100 evenly spaced proposal thresholds between 0 and 1, and use our testing data to compute the cost.\n",
    "2. With your newly computed threshold value, compute the cost of putting this model in production by computing the cost using the validation data. Also provide a confidence interval of the cost using Hoeffdings inequality with a 95% confidence.\n",
    "3. Let 𝑡 be the threshold you found and 𝑓 the model you fitted, if we define the random variable 𝐶 = 30(1 − 1𝑓(𝑋)≥𝑡 )𝑌 + 5(1 − 𝑌 )1𝑓(𝑋)≥𝑡\n",
    "then 𝐶 denotes the cost of a randomly chosen patient. In the above we are estimating 𝔼[𝐶] using the empirical mean. However, since the number of calcifications in the population is fairly small and our classifier probably has a fairly high precision for the 0 class, then 𝐶 should have fairly small variance. Compute the empirical variance of 𝐶 on the validation set. What would be the confidence interval if we used Bennett's inequality instead of Hoeffding in point 6 but with the computed empirical variance as our guess for the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as so\n",
    "import numpy as np\n",
    "data = so.loadmat('data/mammography.mat')\n",
    "np.random.seed(0)\n",
    "shuffle_index = np.arange(0,len(data['X']))\n",
    "np.random.shuffle(shuffle_index)\n",
    "X = data['X'][shuffle_index,:]\n",
    "Y = data['y'][shuffle_index,:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6709, 6) (1677, 6) (2797, 6)\n",
      "(6709,) (1677,) (2797,)\n"
     ]
    }
   ],
   "source": [
    "#Part1\n",
    "# Split the X,Y into three parts, make sure that the sizes are\n",
    "# (6709, 6), (1677, 6), (2797, 6), (6709,), (1677,), (2797,)\n",
    "# Split the data: \n",
    "# 60% for training, 15% for testing, and 25% for validation\n",
    "train_size = int(0.6 * len(X))\n",
    "test_size = int(0.15 * len(X))\n",
    "valid_size = len(X) - train_size - test_size\n",
    "\n",
    "# Split the data into train, test, and validation sets\n",
    "X_train = X[:train_size, :]\n",
    "X_test = X[train_size:train_size + test_size, :]\n",
    "X_valid = X[train_size + test_size:, :]\n",
    "\n",
    "Y_train = Y[:train_size]\n",
    "Y_test = Y[train_size:train_size + test_size]\n",
    "Y_valid = Y[train_size + test_size:]\n",
    "\n",
    "# The sizes of the datasets\n",
    "print(X_train.shape, X_test.shape, X_valid.shape)\n",
    "print(Y_train.shape, Y_test.shape, Y_valid.shape)\n",
    "#X_train, X_test, X_valid, Y_train, Y_test, Y_valid = XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('logreg', LogisticRegression())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part2\n",
    "# Use X_train,Y_train to train a model from sklearn. Make sure it h as the predict_proba function\n",
    "# for instance LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with StandardScaler and LogisticRegression\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),         # Normalize the data\n",
    "    ('logreg', LogisticRegression())      # Logistic Regression Model\n",
    "])\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# The model is now trained and ready to make predictions\n",
    "# You can use model.predict_proba() for probabilistic predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for class 0: (0.98, 0.94)\n",
      "Recall for class 0: (1.0, 0.96)\n",
      "Precision for class 1: (0.7, 0.33)\n",
      "Recall for class 1: (0.36, 0.09)\n"
     ]
    }
   ],
   "source": [
    "#Part3\n",
    "# Compute precision and recall on the test set using\n",
    "\n",
    "# Assuming classification_report_interval is already imported from Utils\n",
    "from Utils import classification_report_interval\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "import re\n",
    "\n",
    "# Function to extract precision and recall from the classification report string\n",
    "def extract_metrics(report):\n",
    "    # Define a regex pattern to extract precision, recall, and their confidence intervals\n",
    "    pattern = r'(\\d)\\s+(\\d\\.\\d{2})\\s*:\\s*\\[(\\d\\.\\d{2}),(\\d\\.\\d{2})\\]\\s+(\\d\\.\\d{2})\\s*:\\s*\\[(\\d\\.\\d{2}),(\\d\\.\\d{2})\\]'\n",
    "    \n",
    "    # Find all matches for the pattern\n",
    "    matches = re.findall(pattern, report)\n",
    "    \n",
    "    # Parse the precision and recall for both classes (0 and 1)\n",
    "    precision0 = (float(matches[0][1]), float(matches[0][2]))\n",
    "    recall0 = (float(matches[0][4]), float(matches[0][5]))\n",
    "    \n",
    "    precision1 = (float(matches[1][1]), float(matches[1][2]))\n",
    "    recall1 = (float(matches[1][4]), float(matches[1][5]))\n",
    "    \n",
    "    return precision0, recall0, precision1, recall1\n",
    "\n",
    "# Get the report from the classification_report_interval function\n",
    "report = classification_report_interval(Y_test, y_pred, alpha=0.05)\n",
    "\n",
    "# Extract precision and recall values\n",
    "precision0, recall0, precision1, recall1 = extract_metrics(report)\n",
    "\n",
    "# Print the extracted precision and recall\n",
    "print(f\"Precision for class 0: {precision0}\")\n",
    "print(f\"Recall for class 0: {recall0}\")\n",
    "print(f\"Precision for class 1: {precision1}\")\n",
    "print(f\"Recall for class 1: {recall1}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cost: 0.48300536672629696\n"
     ]
    }
   ],
   "source": [
    " #Part4\n",
    "\n",
    "def cost(model, threshold, X, Y):\n",
    "    # Get predicted probabilities for the positive class (calcified)\n",
    "    pred_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Apply threshold to get the binary predictions\n",
    "    predictions = (pred_proba >= threshold) * 1\n",
    "    \n",
    "    # Calculate True Positives, False Positives, True Negatives, False Negatives\n",
    "    TP = ((predictions == 1) & (Y == 1)).sum()  # Predicted 1, True 1\n",
    "    FP = ((predictions == 1) & (Y == 0)).sum()  # Predicted 1, True 0\n",
    "    TN = ((predictions == 0) & (Y == 0)).sum()  # Predicted 0, True 0\n",
    "    FN = ((predictions == 0) & (Y == 1)).sum()  # Predicted 0, True 1\n",
    "    \n",
    "    # Define costs\n",
    "    cost_FN = 30  # Cost for False Negative (worst case)\n",
    "    cost_FP = 10  # Cost for False Positive\n",
    "    \n",
    "    # Compute total cost\n",
    "    total_cost = (FP * cost_FP) + (FN * cost_FN)\n",
    "    \n",
    "    # Compute the average cost per sample\n",
    "    average_cost = total_cost / len(Y)\n",
    "    \n",
    "    return average_cost\n",
    "\n",
    "# Example usage:\n",
    "threshold = 0.5  # You can adjust the threshold as needed\n",
    "average_cost = cost(model, threshold, X_test, Y_test)\n",
    "print(f\"Average cost: {average_cost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive cost: 0.6976744186046512\n"
     ]
    }
   ],
   "source": [
    "#Part4\n",
    "# Fill in the naive cost of the model that would classify all as non-calcified on the test set\n",
    "def naive_cost(Y):\n",
    "    # False Negatives: Y=1 (calcified), but predicted as 0 (non-calcified)\n",
    "    FN = (Y == 1).sum()  # All calcified samples will be False Negatives in the naive model\n",
    "    \n",
    "    # Define cost for False Negative\n",
    "    cost_FN = 30  # Cost for False Negative (worst case)\n",
    "    \n",
    "    # Compute total naive cost\n",
    "    total_naive_cost = FN * cost_FN\n",
    "    \n",
    "    # Compute the naive average cost per sample\n",
    "    naive_average_cost = total_naive_cost / len(Y)\n",
    "    \n",
    "    return naive_average_cost\n",
    "\n",
    "# Example usage:\n",
    "naive_cost_value = naive_cost(Y_test)  # Assuming Y_test contains the true labels\n",
    "print(f\"Naive cost: {naive_cost_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.19191919191919193\n",
      "Cost at optimal threshold: 0.3935599284436494\n"
     ]
    }
   ],
   "source": [
    "#Part5\n",
    "\n",
    "# Now, we need to find the optimal threshold that minimizes the cost.\n",
    "# We'll check 100 evenly spaced thresholds between 0 and 1.\n",
    "\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "costs = [cost(model, threshold, X_test, Y_test) for threshold in thresholds]\n",
    "\n",
    "# Find the threshold with the minimum cost\n",
    "optimal_threshold = thresholds[np.argmin(costs)]\n",
    "cost_at_optimal_threshold = min(costs)\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")\n",
    "print(f\"Cost at optimal threshold: {cost_at_optimal_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Part6\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Compute the cost at optimal threshold on the validation set\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Find the optimal threshold (the one with the minimum cost)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m optimal_threshold \u001b[38;5;241m=\u001b[39m thresholds[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcosts\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get the cost at the optimal threshold on the validation set\u001b[39;00m\n\u001b[1;32m      9\u001b[0m cost_at_optimal_threshold_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(costs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1325\u001b[0m, in \u001b[0;36margmin\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "#Part6\n",
    "\n",
    "# Compute the cost at optimal threshold on the validation set\n",
    "\n",
    "# Find the optimal threshold (the one with the minimum cost)\n",
    "optimal_threshold = thresholds[np.argmin(costs)]\n",
    "\n",
    "# Get the cost at the optimal threshold on the validation set\n",
    "cost_at_optimal_threshold_validation = min(costs)\n",
    "\n",
    "# Report the cost interval as a tuple cost_interval = (a,b) cost_interval = XXX\n",
    "# The code below will tell you if you filled in the intervals corre ctly\n",
    "assert(type(cost_interval) == tuple)\n",
    "assert(len(cost_interval) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part7 \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define cost values for False Negatives (FN) and False Positives (FP)\n",
    "cost_FN = 30  # Cost when a calcified sample is classified as non-calcified\n",
    "cost_FP = 10  # Cost when a non-calcified sample is classified as calcified\n",
    "\n",
    "# Ensure the model has been trained and is able to make predictions\n",
    "if hasattr(model, 'predict_proba'):\n",
    "    # Get predicted probabilities for the positive class (calcified)\n",
    "    pred_proba = model.predict_proba(X_valid)[:, 1]  # Probabilities for class 1 (calcified)\n",
    "\n",
    "    # Get binary predictions based on the optimal threshold (which is already computed)\n",
    "    predictions = (pred_proba >= optimal_threshold) * 1\n",
    "\n",
    "    # Calculate True Positives, False Positives, True Negatives, False Negatives\n",
    "    TP = ((predictions == 1) & (Y_valid == 1)).sum()  # Predicted 1, True 1\n",
    "    FP = ((predictions == 1) & (Y_valid == 0)).sum()  # Predicted 1, True 0\n",
    "    TN = ((predictions == 0) & (Y_valid == 0)).sum()  # Predicted 0, True 0\n",
    "    FN = ((predictions == 0) & (Y_valid == 1)).sum()  # Predicted 0, True 1\n",
    "    \n",
    "    # Compute total cost at the optimal threshold\n",
    "    total_cost = (FP * cost_FP) + (FN * cost_FN)\n",
    "    \n",
    "    # Compute average cost per sample\n",
    "    average_cost = total_cost / len(Y_valid)\n",
    "    \n",
    "    # Apply Hoeffding's inequality to compute the confidence interval\n",
    "    n = len(Y_valid)  # Sample size\n",
    "    alpha = 0.05  # 95% confidence\n",
    "    l = np.sqrt(np.log(2 / alpha) / (2 * n))  # Hoeffding's bound for margin of error\n",
    "    \n",
    "    # Compute the confidence interval around the cost\n",
    "    lower_bound = average_cost - l\n",
    "    upper_bound = average_cost + l\n",
    "    cost_interval = (lower_bound, upper_bound)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Cost at optimal threshold on validation set: {average_cost}\")\n",
    "    print(f\"Confidence interval for the cost at 95% confidence: {cost_interval}\")\n",
    "else:\n",
    "    raise ValueError(\"The model does not have the 'predict_proba' method. Ensure it is trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical variance of C: 8.48486854946608\n",
      "Confidence interval for the cost using Bennett's inequality: (0.1739585599378358, 0.47316335640109874)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the cost C for each sample in the validation set\n",
    "def compute_cost(C_pred, Y, t):\n",
    "    return 30 * (1 - C_pred) * Y + 5 * (1 - Y) * C_pred\n",
    "\n",
    "# Ensure that the model has been trained and we can make predictions\n",
    "if hasattr(model, 'predict_proba'):\n",
    "    # Get predicted probabilities for the positive class (calcified)\n",
    "    pred_proba = model.predict_proba(X_valid)[:, 1]  # Probabilities for class 1 (calcified)\n",
    "    \n",
    "    # Get binary predictions based on the optimal threshold (which is already computed)\n",
    "    C_pred = (pred_proba >= optimal_threshold) * 1\n",
    "    \n",
    "    # Compute the cost C for each sample\n",
    "    C = compute_cost(C_pred, Y_valid, optimal_threshold)\n",
    "\n",
    "    # Compute empirical mean of C\n",
    "    C_mean = np.mean(C)\n",
    "\n",
    "    # Compute empirical variance of C\n",
    "    variance_of_C = np.var(C)\n",
    "\n",
    "    # Compute the margin of error for the 95% confidence interval using Bennett's inequality\n",
    "    n = len(C)  # Sample size\n",
    "    alpha = 0.05  # 95% confidence\n",
    "    l = np.sqrt((2 * variance_of_C * np.log(2 / alpha)) / n)\n",
    "\n",
    "    # Compute the confidence interval for the cost using Bennett's inequality\n",
    "    interval_of_C = (C_mean - l, C_mean + l)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Empirical variance of C: {variance_of_C}\")\n",
    "    print(f\"Confidence interval for the cost using Bennett's inequality: {interval_of_C}\")\n",
    "else:\n",
    "    raise ValueError(\"The model does not have the 'predict_proba' method. Ensure it is trained.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
