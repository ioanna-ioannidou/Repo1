{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cbc4fa",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2024/)    \n",
    "## 1MS041, 2024 \n",
    "&copy;2024 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1f24d",
   "metadata": {},
   "source": [
    "## A great set of lectures in Optimisation in next URL\n",
    "\n",
    "You have seen some methods in scientific computing and will see more in your programme.\n",
    "\n",
    "Here we just use some basic methods in numerical optimisation to be able to find the maximum likelihood estimate.\n",
    "\n",
    "Note that in optimisation one is interested in minimising a cost or loss, so we are interested in the following\n",
    "\n",
    "$$\\hat \\theta = \\arg\\min_{\\theta \\in \\mathbf{\\Theta}} - \\log(L(\\theta))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0b501b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"95%\"\n",
       "            height=\"500\"\n",
       "            src=\"https://scipy-lectures.org/advanced/mathematical_optimization/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1043a3ee0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils import showURL\n",
    "showURL(\"https://scipy-lectures.org/advanced/mathematical_optimization/index.html\",500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf814a6a",
   "metadata": {},
   "source": [
    "**2.7.2.1. Getting started: 1D Optimisation**\n",
    "\n",
    "This example is from https://scipy-lectures.org/advanced/mathematical_optimization/index.html#id38\n",
    "\n",
    "> Let’s get started by finding the minimum of the scalar function $f(x)=\\exp(x-0.7)^2$. \n",
    "> `scipy.optimize.minimize_scalar()` uses Brent’s method to find the minimum of a function\n",
    "\n",
    "See [docs for `scipy.optimize.minimize_scalar()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3892616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result.success = True\n",
      "argmin_x of f(x) = 0.6999999997839409\n"
     ]
    }
   ],
   "source": [
    "# Example from https://scipy-lectures.org/advanced/mathematical_optimization/index.html#id38\n",
    "\n",
    "import numpy as np # import numpy for np.methods\n",
    "# import optimize from scipy to do numerical optimization\n",
    "from scipy import optimize\n",
    "\n",
    "#define the function f we want to minimize - see points plot below\n",
    "def f(x):\n",
    "    return -np.exp(-(x - 0.7)**2)\n",
    "\n",
    "#call the optimize.minimize_scalar method to find the minimim, argmin, etc.\n",
    "result = optimize.minimize_scalar(f)\n",
    "\n",
    "print (\"result.success = \" + str(result.success)) # check if solver was successful\n",
    "\n",
    "# argmin, i.e., the x that minimizes f(x)\n",
    "x_min = result.x\n",
    "\n",
    "# note argmin is close to the true argmin at 0.7\n",
    "print(\"argmin_x of f(x) = \" + str(x_min)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "925b17e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa638b0e1f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xValuesToPlot=np.arange(-2.0,2.0,0.05)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(xValuesToPlot,f(xValuesToPlot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93ac7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 12 9\n"
     ]
    }
   ],
   "source": [
    "# the value of the function's minimum, number of function evaluations, number of iterations\n",
    "print (result.fun, result.nfev, result.nit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a693203e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " message: \n",
       "          Optimization terminated successfully;\n",
       "          The returned value satisfies the termination criteria\n",
       "          (using xtol = 1.48e-08 )\n",
       " success: True\n",
       "     fun: -1.0\n",
       "       x: 0.6999999997839409\n",
       "     nit: 9\n",
       "    nfev: 12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result # we can see result directly too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6730f",
   "metadata": {},
   "source": [
    "### Bounded 1D Optimisation\n",
    "\n",
    "In MLe problems we often want to optimise or minimise $f(x)$ while constraining of bounding $x$ to be inside an interval, say $[0,1]$. This is possible using `method='bounded'` in our previous example. The implementation uses Brent's method under the set constraints. \n",
    "\n",
    "See [docs for `scipy.optimize.minimize_scalar()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e364c5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " message: Solution found.\n",
       " success: True\n",
       "  status: 0\n",
       "     fun: -0.9999999999997916\n",
       "       x: 0.6999995435529417\n",
       "     nit: 8\n",
       "    nfev: 8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "def f(x):\n",
    "    return -np.exp(-(x - 0.7)**2)\n",
    "\n",
    "initial_x = 0.5 # this is where we are initialising the iterative search\n",
    "boundedResult = optimize.minimize_scalar(f, initial_x, bounds=(0, 1), method='bounded')\n",
    "boundedResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892af80",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimate using Optimisation\n",
    "\n",
    "Let us apply the above idea in **Bounded 1D Optimisation** to find the MLe for statistical experiments from $n$ IID samples that are assumed to drawn from a random variable with only one real-valued parameter that satisfies some given  constraints.\n",
    "\n",
    "We can thus get the MLe by minimising the negative log likelihood function *numerically* using *floating-point numbers* via `numpy` ans `scipy` libraries/packages (such methods are also  called *scientific computing*) .\n",
    "\n",
    "This approach is easier in a sense as you only need the likelhood expression, however, it comes at a price. The solutions do not in general have guarantees unless various conditions are satisfied. However, a lot of successful methods in statistical machine learning and AI are based on such methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13ff21",
   "metadata": {},
   "source": [
    "## Multi-dimensional Constrained Optimisation\n",
    "\n",
    "The following example is from [2.7.7.1. Box bounds](https://scipy-lectures.org/advanced/mathematical_optimization/index.html#id54) of [scipy lectures on mathematical optimisation](https://scipy-lectures.org/advanced/mathematical_optimization/index.html).\n",
    "\n",
    "We want to find the argument that minimises the function $f$ of two variables $(x_1,x_2)$\n",
    "$$\n",
    "argmin_{(x_1,x_2) \\in [-1.5,1.5]^2} f(x_1,x_2) = \\sqrt{\\left( (x_1 - 3)^2 + (x_2 - 2)^2 \\right)}\n",
    "$$\n",
    "but while constraining $(x_1,x_2)$ to lie within the bounding box given by the square $[-1.5,1.5]^2 := [-1.5,1.5] \\times [-1.5,1.5]$.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://scipy-lectures.org/_images/sphx_glr_plot_constraints_002.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "We can use this iterative numerical method called `optimize.minimize` to find the MLe of statistical experiments that have more than one parameter using:\n",
    "\n",
    "- the numerical expression for a function `f` we want to minimise\n",
    "  - for us, `f` will be `negLogLkl`, the negative log likelihood function (or any other loss for that matter), for a given `dataset`\n",
    "- specifying `parameter_bounding_box`, the appropriate bounding boxes for the parameters (so the likelihood is well-defined)\n",
    "- specifying `initial_arguments` for the initial values for the parameters in the iterative `optimize.minimize` method\n",
    "  - make sure the initial value is within the bounding box!\n",
    "\n",
    "You should have seen these ideas in some form in your scientific computing courses, if not, don't worry as you will see them soon in more advanced scientific computing courses you will be taking in the future.\n",
    "\n",
    "If you want to use the source to deepen your understanding through a great set of codes and lecture notes that will help you understand what is going on when we call `optimize.minimize` method, then please feel free to dive here:\n",
    "\n",
    "- [https://scipy-lectures.org/advanced/mathematical_optimization/auto_examples/plot_gradient_descent.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-gradient-descent-py](https://scipy-lectures.org/advanced/mathematical_optimization/auto_examples/plot_gradient_descent.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-gradient-descent-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "571716af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 1.5811388300841898\n",
       "        x: [ 1.500e+00  1.500e+00]\n",
       "      nit: 2\n",
       "      jac: [-9.487e-01 -3.162e-01]\n",
       "     nfev: 9\n",
       "     njev: 3\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "# define the objective/cost/loss function we want to minimise\n",
    "def f(x):\n",
    "    return np.sqrt((x[0] - 3)**2 + (x[1] - 2)**2)\n",
    "\n",
    "# multi-dimensional optimisation is syntactically similar to 1D, \n",
    "# but we are using Gradient and Hessian information from numerical evaluation of f to \n",
    "# iteratively improve the solution along the steepest direction, etc. \n",
    "# It 'LBFGS' method you will see in scientific computing\n",
    "parameter_bounding_box=((-1.5, 1.5), (-1.5, 1.5)) # specify the constraints for each parameter\n",
    "initial_arguments = np.array([0, 0]) # point in 2D to initialise the minimize algorithm\n",
    "optimize.minimize(f, initial_arguments, bounds=parameter_bounding_box) # just call the minimize method!\n",
    "# notuce that the argmin x = (1.5,1,5) within the bounding-box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e881b85",
   "metadata": {},
   "source": [
    "\n",
    "1. **`message:`** The optimization converged because the gradient norm was below the tolerance.  \n",
    "2. **`success:`** The optimization was successful.  \n",
    "3. **`status:`** Status code `0` means successful convergence.  \n",
    "4. **`fun:`** The objective function value at the solution is \\(1.5811\\).  \n",
    "5. **`x:`** The solution point is \\([1.5, 1.5]\\).  \n",
    "6. **`nit:`** The algorithm completed in 2 iterations.  \n",
    "7. **`jac:`** The gradient at the solution is \\([-0.9487, -0.3162]\\), close to zero.  \n",
    "8. **`nfev:`** The objective function was evaluated 9 times.  \n",
    "9. **`njev:`** The gradient was evaluated 3 times.  \n",
    "10. **`hess_inv:`** An approximation of the inverse Hessian matrix is available for curvature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651da6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "lx_course_instance": "2024",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
