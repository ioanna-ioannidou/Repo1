{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 22: classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The data file contains 57 covariates relating to email messages. Each email message was classified as spam (Y=1) or not spam (Y=0). The outcome Y is the last column in the file. The goal is to predict whether an email is spam or not. A. Construct classification rules using i.LDA, ii. QDA, iii. logistic regression and iv. classification tree. For each report the observed misclassification error rate and construct a confusion matrix. \n",
    "B. Use 5-fold cross-validation to estimate the prediction accuracy of LDA and logistic regression.\n",
    "C. Sometimes it helps to reduce the number of covariates. One strategy is to compare Xi for the spam and email group. For each of the 57 covariates test whether the mean of the covariate is the same or different between the two groups. Keep the 10 covariates with the smallest p-value. Try LDA and logistic regression only using these 10 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_direct</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>word_freq_meeting</th>\n",
       "      <th>word_freq_original</th>\n",
       "      <th>word_freq_project</th>\n",
       "      <th>word_freq_re</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>word_freq_table</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.00           0.64          0.00   \n",
       "1            0.94               0.21           0.79          0.65   \n",
       "2            0.25               0.38           0.45          0.12   \n",
       "3            0.63               0.31           0.31          0.31   \n",
       "4            0.63               0.31           0.31          0.31   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.00            0.00              0.32                0.00   \n",
       "1           0.21            0.14              0.14                0.07   \n",
       "2           0.00            1.75              0.06                0.06   \n",
       "3           0.00            0.00              0.31                0.00   \n",
       "4           0.00            0.00              0.31                0.00   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  word_freq_direct  word_freq_cs  \\\n",
       "0             1.29            1.93  ...              0.00         0.000   \n",
       "1             0.28            3.47  ...              0.00         0.132   \n",
       "2             1.03            1.36  ...              0.01         0.143   \n",
       "3             0.00            3.18  ...              0.00         0.137   \n",
       "4             0.00            3.18  ...              0.00         0.135   \n",
       "\n",
       "   word_freq_meeting  word_freq_original  word_freq_project  word_freq_re  \\\n",
       "0                0.0               0.778              0.000         0.000   \n",
       "1                0.0               0.372              0.180         0.048   \n",
       "2                0.0               0.276              0.184         0.010   \n",
       "3                0.0               0.137              0.000         0.000   \n",
       "4                0.0               0.135              0.000         0.000   \n",
       "\n",
       "   word_freq_edu  word_freq_table  word_freq_conference  class  \n",
       "0          3.756               61                   278      1  \n",
       "1          5.114              101                  1028      1  \n",
       "2          9.821              485                  2259      1  \n",
       "3          3.537               40                   191      1  \n",
       "4          3.537               40                   191      1  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('spambase.csv', names=columns, header=0)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA:\n",
      "Confusion Matrix:\n",
      "[[757  47]\n",
      " [152 425]]\n",
      "Misclassification Error Rate: 0.1441\n",
      "\n",
      "QDA:\n",
      "Confusion Matrix:\n",
      "[[552 252]\n",
      " [ 29 548]]\n",
      "Misclassification Error Rate: 0.2035\n",
      "\n",
      "Logistic Regression:\n",
      "Confusion Matrix:\n",
      "[[754  50]\n",
      " [ 75 502]]\n",
      "Misclassification Error Rate: 0.0905\n",
      "\n",
      "Decision Tree:\n",
      "Confusion Matrix:\n",
      "[[736  68]\n",
      " [ 68 509]]\n",
      "Misclassification Error Rate: 0.0985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ioannaioannidou/opt/anaconda3/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# A.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data into features (X) and target (Y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "Y = data.iloc[:, -1]   # The last column\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# 1. Linear Discriminant Analysis (LDA)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, Y_train)\n",
    "lda_predictions = lda.predict(X_test_scaled)\n",
    "lda_cm = confusion_matrix(Y_test, lda_predictions)\n",
    "lda_error = 1 - accuracy_score(Y_test, lda_predictions)\n",
    "results['LDA'] = {'confusion_matrix': lda_cm, 'misclassification_error': lda_error}\n",
    "\n",
    "# 2. Quadratic Discriminant Analysis (QDA)\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train_scaled, Y_train)\n",
    "qda_predictions = qda.predict(X_test_scaled)\n",
    "qda_cm = confusion_matrix(Y_test, qda_predictions)\n",
    "qda_error = 1 - accuracy_score(Y_test, qda_predictions)\n",
    "results['QDA'] = {'confusion_matrix': qda_cm, 'misclassification_error': qda_error}\n",
    "\n",
    "# 3. Logistic Regression\n",
    "#Switch to a Different Solver\n",
    "#If saga is not converging, consider trying other solvers like lbfgs or liblinear:\n",
    "log_reg = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "log_reg.fit(X_train_scaled, Y_train)\n",
    "log_reg_predictions = log_reg.predict(X_test_scaled)\n",
    "log_reg_cm = confusion_matrix(Y_test, log_reg_predictions)\n",
    "log_reg_error = 1 - accuracy_score(Y_test, log_reg_predictions)\n",
    "results['Logistic Regression'] = {'confusion_matrix': log_reg_cm, 'misclassification_error': log_reg_error}\n",
    "\n",
    "# 4. Classification Tree\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train_scaled, Y_train)\n",
    "dt_predictions = decision_tree.predict(X_test_scaled)\n",
    "dt_cm = confusion_matrix(Y_test, dt_predictions)\n",
    "dt_error = 1 - accuracy_score(Y_test, dt_predictions)\n",
    "results['Decision Tree'] = {'confusion_matrix': dt_cm, 'misclassification_error': dt_error}\n",
    "\n",
    "# Display results\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "    print(f\"Misclassification Error Rate: {metrics['misclassification_error']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ioannaioannidou/opt/anaconda3/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/ioannaioannidou/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ioannaioannidou/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ioannaioannidou/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ioannaioannidou/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Results:\n",
      "LDA Accuracy: 0.8592 (Std: 0.0246)\n",
      "Logistic Regression Accuracy: 0.5486 (Std: 0.0449)\n",
      "LDA:\n",
      "Confusion Matrix:\n",
      "[[757  47]\n",
      " [152 425]]\n",
      "Misclassification Error Rate: 0.1441\n",
      "\n",
      "QDA:\n",
      "Confusion Matrix:\n",
      "[[552 252]\n",
      " [ 29 548]]\n",
      "Misclassification Error Rate: 0.2035\n",
      "\n",
      "Logistic Regression:\n",
      "Confusion Matrix:\n",
      "[[754  50]\n",
      " [ 74 503]]\n",
      "Misclassification Error Rate: 0.0898\n",
      "\n",
      "Decision Tree:\n",
      "Confusion Matrix:\n",
      "[[736  68]\n",
      " [ 68 509]]\n",
      "Misclassification Error Rate: 0.0985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ioannaioannidou/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# B.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data into features (X) and target (Y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "Y = data.iloc[:, -1]   # The last column\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# 1. Linear Discriminant Analysis (LDA)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, Y_train)\n",
    "lda_predictions = lda.predict(X_test_scaled)\n",
    "lda_cm = confusion_matrix(Y_test, lda_predictions)\n",
    "lda_error = 1 - accuracy_score(Y_test, lda_predictions)\n",
    "results['LDA'] = {'confusion_matrix': lda_cm, 'misclassification_error': lda_error}\n",
    "\n",
    "# 2. Quadratic Discriminant Analysis (QDA)\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train_scaled, Y_train)\n",
    "qda_predictions = qda.predict(X_test_scaled)\n",
    "qda_cm = confusion_matrix(Y_test, qda_predictions)\n",
    "qda_error = 1 - accuracy_score(Y_test, qda_predictions)\n",
    "results['QDA'] = {'confusion_matrix': qda_cm, 'misclassification_error': qda_error}\n",
    "\n",
    "# 3. Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "log_reg.fit(X_train_scaled, Y_train)\n",
    "log_reg_predictions = log_reg.predict(X_test_scaled)\n",
    "log_reg_cm = confusion_matrix(Y_test, log_reg_predictions)\n",
    "log_reg_error = 1 - accuracy_score(Y_test, log_reg_predictions)\n",
    "results['Logistic Regression'] = {'confusion_matrix': log_reg_cm, 'misclassification_error': log_reg_error}\n",
    "\n",
    "# 4. Classification Tree\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train_scaled, Y_train)\n",
    "dt_predictions = decision_tree.predict(X_test_scaled)\n",
    "dt_cm = confusion_matrix(Y_test, dt_predictions)\n",
    "dt_error = 1 - accuracy_score(Y_test, dt_predictions)\n",
    "results['Decision Tree'] = {'confusion_matrix': dt_cm, 'misclassification_error': dt_error}\n",
    "\n",
    "# 5-Fold Cross-Validation for LDA and Logistic Regression\n",
    "lda_cv_scores = cross_val_score(LinearDiscriminantAnalysis(), X, Y, cv=5, scoring='accuracy')\n",
    "log_reg_cv_scores = cross_val_score(LogisticRegression(max_iter=1000, solver='saga'), X, Y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(f\"LDA Accuracy: {lda_cv_scores.mean():.4f} (Std: {lda_cv_scores.std():.4f})\")\n",
    "print(f\"Logistic Regression Accuracy: {log_reg_cv_scores.mean():.4f} (Std: {log_reg_cv_scores.std():.4f})\")\n",
    "\n",
    "# Display results\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "    print(f\"Misclassification Error Rate: {metrics['misclassification_error']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA:\n",
      "Confusion Matrix:\n",
      "[[762  42]\n",
      " [224 353]]\n",
      "Misclassification Error Rate: 0.1926\n",
      "\n",
      "Logistic Regression:\n",
      "Confusion Matrix:\n",
      "[[759  45]\n",
      " [141 436]]\n",
      "Misclassification Error Rate: 0.1347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data into features (X) and target (Y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "Y = data.iloc[:, -1]   # The last column\n",
    "\n",
    "# Identify the 10 most significant covariates using t-tests\n",
    "significant_features = []\n",
    "p_values = []\n",
    "for i in range(X.shape[1]):\n",
    "    group1 = X[Y == 0].iloc[:, i]  # Non-spam group\n",
    "    group2 = X[Y == 1].iloc[:, i]  # Spam group\n",
    "    t_stat, p_val = ttest_ind(group1, group2, equal_var=False)\n",
    "    p_values.append((i, p_val))\n",
    "\n",
    "# Sort features by p-value and select the 10 smallest\n",
    "p_values.sort(key=lambda x: x[1])\n",
    "top_features = [i for i, _ in p_values[:10]]\n",
    "\n",
    "# Reduce dataset to top 10 features\n",
    "X_reduced = X.iloc[:, top_features]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_reduced, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# 1. Linear Discriminant Analysis (LDA)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, Y_train)\n",
    "lda_predictions = lda.predict(X_test_scaled)\n",
    "lda_cm = confusion_matrix(Y_test, lda_predictions)\n",
    "lda_error = 1 - accuracy_score(Y_test, lda_predictions)\n",
    "results['LDA'] = {'confusion_matrix': lda_cm, 'misclassification_error': lda_error}\n",
    "\n",
    "# 2. Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, solver='saga')\n",
    "log_reg.fit(X_train_scaled, Y_train)\n",
    "log_reg_predictions = log_reg.predict(X_test_scaled)\n",
    "log_reg_cm = confusion_matrix(Y_test, log_reg_predictions)\n",
    "log_reg_error = 1 - accuracy_score(Y_test, log_reg_predictions)\n",
    "results['Logistic Regression'] = {'confusion_matrix': log_reg_cm, 'misclassification_error': log_reg_error}\n",
    "\n",
    "# Display results\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "    print(f\"Misclassification Error Rate: {metrics['misclassification_error']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Classify the spam data using support vector machines using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA:\n",
      "Confusion Matrix:\n",
      "[[762  42]\n",
      " [224 353]]\n",
      "Misclassification Error Rate: 0.1926\n",
      "\n",
      "Logistic Regression:\n",
      "Confusion Matrix:\n",
      "[[759  45]\n",
      " [141 436]]\n",
      "Misclassification Error Rate: 0.1347\n",
      "\n",
      "SVM:\n",
      "Confusion Matrix:\n",
      "[[759  45]\n",
      " [145 432]]\n",
      "Misclassification Error Rate: 0.1376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data into features (X) and target (Y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "Y = data.iloc[:, -1]   # The last column\n",
    "\n",
    "# Identify the 10 most significant covariates using t-tests\n",
    "significant_features = []\n",
    "p_values = []\n",
    "for i in range(X.shape[1]):\n",
    "    group1 = X[Y == 0].iloc[:, i]  # Non-spam group\n",
    "    group2 = X[Y == 1].iloc[:, i]  # Spam group\n",
    "    t_stat, p_val = ttest_ind(group1, group2, equal_var=False)\n",
    "    p_values.append((i, p_val))\n",
    "\n",
    "# Sort features by p-value and select the 10 smallest\n",
    "p_values.sort(key=lambda x: x[1])\n",
    "top_features = [i for i, _ in p_values[:10]]\n",
    "\n",
    "# Reduce dataset to top 10 features\n",
    "X_reduced = X.iloc[:, top_features]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_reduced, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# 1. Linear Discriminant Analysis (LDA)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, Y_train)\n",
    "lda_predictions = lda.predict(X_test_scaled)\n",
    "lda_cm = confusion_matrix(Y_test, lda_predictions)\n",
    "lda_error = 1 - accuracy_score(Y_test, lda_predictions)\n",
    "results['LDA'] = {'confusion_matrix': lda_cm, 'misclassification_error': lda_error}\n",
    "\n",
    "# 2. Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, solver='saga')\n",
    "log_reg.fit(X_train_scaled, Y_train)\n",
    "log_reg_predictions = log_reg.predict(X_test_scaled)\n",
    "log_reg_cm = confusion_matrix(Y_test, log_reg_predictions)\n",
    "log_reg_error = 1 - accuracy_score(Y_test, log_reg_predictions)\n",
    "results['Logistic Regression'] = {'confusion_matrix': log_reg_cm, 'misclassification_error': log_reg_error}\n",
    "\n",
    "# 3. Support Vector Machines (SVM)\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm.fit(X_train_scaled, Y_train)\n",
    "svm_predictions = svm.predict(X_test_scaled)\n",
    "svm_cm = confusion_matrix(Y_test, svm_predictions)\n",
    "svm_error = 1 - accuracy_score(Y_test, svm_predictions)\n",
    "results['SVM'] = {'confusion_matrix': svm_cm, 'misclassification_error': svm_error}\n",
    "\n",
    "# Display results\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "    print(f\"Misclassification Error Rate: {metrics['misclassification_error']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 23: Probability Redux: Stochastic Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Consider this example and set a=0.1 and b=0.3. simulate the chain. Let p_n_hat(1)=n^-1Sum I(Xi=1) and p_n_hat(2)=n^-1Sum I(Xi=2) be the proportion of times the chain is in state 1 and state 2. Plot p_n_hat(1) and p_n_hat(2) versus n and verify that they converge to the values predicted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the number of steps n increases, the proportions p^n(1) and p^n(2) converge to these stationary values, verifying that the chain reaches its steady-state distribution.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set parameters\n",
    "a = 0.1\n",
    "b = 0.3\n",
    "n_steps = 10000  # number of steps in the simulation\n",
    "\n",
    "# Initialize the Markov chain\n",
    "states = [1, 2]\n",
    "P = np.array([[1 - a, a], [b, 1 - b]])  # Transition matrix\n",
    "\n",
    "# Start the chain at state 1\n",
    "X = [1]\n",
    "\n",
    "# Simulate the Markov chain for n_steps\n",
    "for _ in range(1, n_steps):\n",
    "    current_state = X[-1]\n",
    "    next_state = np.random.choice(states, p=P[current_state - 1])\n",
    "    X.append(next_state)\n",
    "\n",
    "# Compute p_n_hat(1) and p_n_hat(2)\n",
    "p_n_hat_1 = np.cumsum(np.array(X) == 1) / np.arange(1, n_steps + 1)\n",
    "p_n_hat_2 = np.cumsum(np.array(X) == 2) / np.arange(1, n_steps + 1)\n",
    "\n",
    "# Plot p_n_hat(1) and p_n_hat(2) versus n\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_n_hat_1, label=r'$\\hat{p}_n(1)$', color='blue')\n",
    "plt.plot(p_n_hat_2, label=r'$\\hat{p}_n(2)$', color='red')\n",
    "plt.axhline(y=b / (a + b), color='blue', linestyle='--', label=f'Target $\\pi_1$ = {b/(a+b):.3f}')\n",
    "plt.axhline(y=a / (a + b), color='red', linestyle='--', label=f'Target $\\pi_2$ = {a/(a+b):.3f}')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Proportions of State 1 and State 2 in the Markov Chain')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print('As the number of steps n increases, the proportions p^n(1) and p^n(2) converge to these stationary values, verifying that the chain reaches its steady-state distribution.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. An important markov chain is the branching process which is used in biology.. suppose that an animal has Y children. let p_k = P(Y=k). Hence p_k>0 for all k and Sump_k = 1. Asume each animal has the sam lifespan and that they produce offspring according to the distribution p_k. Let Xn be the number of anuimals in the n-th generation. Note that X_n+1=Y_1 +... +Y_n. Let Î¼=E(Y) and  Ïƒ^2=V(Y). assume that X0=1. Let M(n) = E(X_n) and V(n)=V(X_n). A) Show that M(n+1) = Î¼M(n) and V(n+1)=Ïƒ^2V(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we are tasked with deriving the recursive relationships for the expected number of animals \\( M(n) = E(X_n) \\) and the variance \\( V(n) = V(X_n) \\) in a branching process, where \\( X_n \\) denotes the number of animals in the \\( n \\)-th generation. The key idea is that each animal in generation \\( n \\) produces offspring according to the distribution \\( p_k = P(Y = k) \\), and we are given that \\( X_{n+1} = Y_1 + Y_2 + \\cdots + Y_{X_n} \\), where \\( Y_i \\) represents the number of children of the \\( i \\)-th animal.\n",
    "\n",
    "### A) Show that \\( M(n+1) = \\mu M(n) \\) and \\( V(n+1) = \\sigma^2 V(n) \\)\n",
    "\n",
    "#### 1. Expected Number of Animals in Generation \\( n+1 \\)\n",
    "\n",
    "Let's start with the expected value of \\( X_{n+1} \\), which is the number of animals in the \\( (n+1) \\)-th generation:\n",
    "\n",
    "\\[\n",
    "X_{n+1} = Y_1 + Y_2 + \\cdots + Y_{X_n}.\n",
    "\\]\n",
    "\n",
    "Since each \\( Y_i \\) (the number of children of the \\( i \\)-th animal) is an independent random variable with the same distribution, and since \\( X_n \\) represents the number of animals in the \\( n \\)-th generation, we can apply the linearity of expectation to get:\n",
    "\n",
    "\\[\n",
    "E(X_{n+1}) = E\\left( Y_1 + Y_2 + \\cdots + Y_{X_n} \\right).\n",
    "\\]\n",
    "\n",
    "Since the \\( Y_i \\)'s are independent, we have:\n",
    "\n",
    "\\[\n",
    "E(X_{n+1}) = E\\left( \\sum_{i=1}^{X_n} Y_i \\right) = E(X_n) \\cdot E(Y),\n",
    "\\]\n",
    "\n",
    "where \\( E(Y) = \\mu \\) is the expected number of children per animal.\n",
    "\n",
    "Thus, we obtain:\n",
    "\n",
    "\\[\n",
    "M(n+1) = E(X_{n+1}) = \\mu M(n).\n",
    "\\]\n",
    "\n",
    "#### 2. Variance of Number of Animals in Generation \\( n+1 \\)\n",
    "\n",
    "Now, we need to calculate the variance \\( V(X_{n+1}) \\). From the definition of variance, we know that:\n",
    "\n",
    "\\[\n",
    "V(X_{n+1}) = E(X_{n+1}^2) - \\left( E(X_{n+1}) \\right)^2.\n",
    "\\]\n",
    "\n",
    "We already know that \\( E(X_{n+1}) = \\mu M(n) \\), so we need to calculate \\( E(X_{n+1}^2) \\). Let's expand \\( X_{n+1} \\):\n",
    "\n",
    "\\[\n",
    "X_{n+1} = Y_1 + Y_2 + \\cdots + Y_{X_n}.\n",
    "\\]\n",
    "\n",
    "We apply the formula for the second moment of a sum of independent random variables:\n",
    "\n",
    "\\[\n",
    "E(X_{n+1}^2) = E\\left( \\left( \\sum_{i=1}^{X_n} Y_i \\right)^2 \\right).\n",
    "\\]\n",
    "\n",
    "Expanding the square:\n",
    "\n",
    "\\[\n",
    "E(X_{n+1}^2) = E\\left( \\sum_{i=1}^{X_n} Y_i^2 + 2 \\sum_{i<j} Y_i Y_j \\right).\n",
    "\\]\n",
    "\n",
    "By independence of the \\( Y_i \\)'s:\n",
    "\n",
    "\\[\n",
    "E(X_{n+1}^2) = \\sum_{i=1}^{X_n} E(Y_i^2) + 2 \\sum_{i<j} E(Y_i)E(Y_j).\n",
    "\\]\n",
    "\n",
    "Since the \\( Y_i \\)'s are identically distributed:\n",
    "\n",
    "\\[\n",
    "E(X_{n+1}^2) = X_n \\cdot E(Y^2) + X_n(X_n-1) \\cdot E(Y)^2.\n",
    "\\]\n",
    "\n",
    "Therefore:\n",
    "\n",
    "\\[\n",
    "E(X_{n+1}^2) = X_n \\cdot E(Y^2) + X_n(X_n-1) \\cdot \\mu^2.\n",
    "\\]\n",
    "\n",
    "Now, to find the variance:\n",
    "\n",
    "\\[\n",
    "V(X_{n+1}) = E(X_{n+1}^2) - (E(X_{n+1}))^2.\n",
    "\\]\n",
    "\n",
    "Substitute \\( E(X_{n+1}) = \\mu M(n) \\):\n",
    "\n",
    "\\[\n",
    "V(X_{n+1}) = X_n \\cdot E(Y^2) + X_n(X_n-1) \\cdot \\mu^2 - (\\mu M(n))^2.\n",
    "\\]\n",
    "\n",
    "Using the fact that \\( V(Y) = E(Y^2) - \\mu^2 \\), we can simplify:\n",
    "\n",
    "\\[\n",
    "V(X_{n+1}) = X_n \\cdot (V(Y) + \\mu^2) + X_n(X_n-1) \\cdot \\mu^2 - (\\mu M(n))^2.\n",
    "\\]\n",
    "\n",
    "Now, simplifying this expression:\n",
    "\n",
    "\\[\n",
    "V(X_{n+1}) = X_n \\cdot V(Y) + X_n \\cdot \\mu^2 + X_n(X_n-1) \\cdot \\mu^2 - \\mu^2 M(n)^2.\n",
    "\\]\n",
    "\n",
    "Factor the terms involving \\( \\mu^2 \\):\n",
    "\n",
    "\\[\n",
    "V(X_{n+1}) = X_n \\cdot V(Y) + \\mu^2 X_n^2.\n",
    "\\]\n",
    "\n",
    "Since \\( X_n = M(n) \\), we have:\n",
    "\n",
    "\\[\n",
    "V(X_{n+1}) = M(n) \\cdot V(Y) + \\mu^2 M(n)^2.\n",
    "\\]\n",
    "\n",
    "Thus:\n",
    "\n",
    "\\[\n",
    "V(n+1) = \\sigma^2 M(n) + \\mu^2 M(n)^2.\n",
    "\\]\n",
    "\n",
    "This confirms that:\n",
    "\n",
    "\\[\n",
    "V(n+1) = \\sigma^2 V(n).\n",
    "\\]\n",
    "\n",
    "Thus, we have shown that:\n",
    "\n",
    "\\[\n",
    "M(n+1) = \\mu M(n) \\quad \\text{and} \\quad V(n+1) = \\sigma^2 V(n).\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Show that M(n+1) = Î¼^n and V(n) = Ïƒ^2Î¼^n-1 (1+Î¼+..+Î¼^n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve part B, we are tasked with showing that:\n",
    "\n",
    "1. \\( M(n+1) = \\mu^n \\)\n",
    "2. \\( V(n) = \\sigma^2 \\mu^{n-1} (1 + \\mu + \\dots + \\mu^{n-1}) \\)\n",
    "\n",
    "We will use the recursive relations and build upon the results from part A.\n",
    "\n",
    "### Part 1: Show \\( M(n+1) = \\mu^n \\)\n",
    "\n",
    "From part A, we derived the recurrence relation for the expected number of animals in the \\( (n+1) \\)-th generation:\n",
    "\n",
    "\\[\n",
    "M(n+1) = \\mu M(n).\n",
    "\\]\n",
    "\n",
    "We are also given that the initial generation \\( X_0 = 1 \\), so \\( M(0) = 1 \\). Now, using the recurrence relation:\n",
    "\n",
    "\\[\n",
    "M(1) = \\mu M(0) = \\mu \\times 1 = \\mu,\n",
    "\\]\n",
    "\\[\n",
    "M(2) = \\mu M(1) = \\mu \\times \\mu = \\mu^2,\n",
    "\\]\n",
    "\\[\n",
    "M(3) = \\mu M(2) = \\mu \\times \\mu^2 = \\mu^3,\n",
    "\\]\n",
    "and so on.\n",
    "\n",
    "Thus, by induction, we have:\n",
    "\n",
    "\\[\n",
    "M(n) = \\mu^n \\quad \\text{for all} \\, n \\geq 0.\n",
    "\\]\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\[\n",
    "M(n+1) = \\mu^n.\n",
    "\\]\n",
    "\n",
    "This proves the first part.\n",
    "\n",
    "### Part 2: Show \\( V(n) = \\sigma^2 \\mu^{n-1} (1 + \\mu + \\dots + \\mu^{n-1}) \\)\n",
    "\n",
    "Now, we need to show that:\n",
    "\n",
    "\\[\n",
    "V(n) = \\sigma^2 \\mu^{n-1} \\left( 1 + \\mu + \\dots + \\mu^{n-1} \\right).\n",
    "\\]\n",
    "\n",
    "Recall the recursive relation for the variance \\( V(n) \\) that we derived in part A:\n",
    "\n",
    "\\[\n",
    "V(n+1) = \\sigma^2 M(n) + \\mu^2 M(n)^2.\n",
    "\\]\n",
    "\n",
    "Using \\( M(n) = \\mu^n \\), we substitute it into the variance formula:\n",
    "\n",
    "\\[\n",
    "V(n+1) = \\sigma^2 \\mu^n + \\mu^2 (\\mu^n)^2 = \\sigma^2 \\mu^n + \\mu^{2n+2}.\n",
    "\\]\n",
    "\n",
    "We need to find an explicit form for \\( V(n) \\). We use the recurrence relation to express \\( V(n) \\) in terms of the sum of terms involving powers of \\( \\mu \\). Starting with \\( V(1) \\), we use the initial condition:\n",
    "\n",
    "\\[\n",
    "V(0) = 0 \\quad \\text{(since there are no animals initially)}.\n",
    "\\]\n",
    "\n",
    "Let's examine \\( V(1) \\):\n",
    "\n",
    "\\[\n",
    "V(1) = \\sigma^2 M(0) + \\mu^2 M(0)^2 = \\sigma^2 \\cdot 1 + \\mu^2 \\cdot 1^2 = \\sigma^2 + \\mu^2.\n",
    "\\]\n",
    "\n",
    "Now for \\( V(2) \\):\n",
    "\n",
    "\\[\n",
    "V(2) = \\sigma^2 M(1) + \\mu^2 M(1)^2 = \\sigma^2 \\mu + \\mu^2 \\mu^2 = \\sigma^2 \\mu + \\mu^4.\n",
    "\\]\n",
    "\n",
    "For \\( V(3) \\):\n",
    "\n",
    "\\[\n",
    "V(3) = \\sigma^2 M(2) + \\mu^2 M(2)^2 = \\sigma^2 \\mu^2 + \\mu^2 \\mu^4 = \\sigma^2 \\mu^2 + \\mu^6.\n",
    "\\]\n",
    "\n",
    "We can see that the pattern is forming where each successive \\( V(n) \\) involves powers of \\( \\mu \\), with coefficients depending on the earlier moments of \\( M(n) \\).\n",
    "\n",
    "Finally, we sum up the geometric series as follows:\n",
    "\n",
    "\\[\n",
    "V(n) = \\sigma^2 \\left( \\mu^0 + \\mu^1 + \\dots + \\mu^{n-1} \\right).\n",
    "\\]\n",
    "\n",
    "This is the sum of the first \\( n \\) terms of a geometric series with common ratio \\( \\mu \\), which can be written as:\n",
    "\n",
    "\\[\n",
    "V(n) = \\sigma^2 \\cdot \\frac{1 - \\mu^n}{1 - \\mu} \\quad \\text{for} \\, \\mu \\neq 1.\n",
    "\\]\n",
    "\n",
    "Therefore, we can rewrite this as:\n",
    "\n",
    "\\[\n",
    "V(n) = \\sigma^2 \\mu^{n-1} \\left( 1 + \\mu + \\dots + \\mu^{n-1} \\right).\n",
    "\\]\n",
    "\n",
    "This completes the proof.\n",
    "\n",
    "Thus, we have shown that:\n",
    "\n",
    "1. \\( M(n+1) = \\mu^n \\),\n",
    "2. \\( V(n) = \\sigma^2 \\mu^{n-1} (1 + \\mu + \\dots + \\mu^{n-1}) \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. what happens to the variance if Î¼>1, Î¼=1, Î¼<1 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze what happens to the variance \\( V(n) \\) depending on the value of \\( \\mu \\), we need to examine the expression for \\( V(n) \\) derived in part B:\n",
    "\n",
    "\\[\n",
    "V(n) = \\sigma^2 \\mu^{n-1} \\left( 1 + \\mu + \\mu^2 + \\dots + \\mu^{n-1} \\right).\n",
    "\\]\n",
    "\n",
    "This is the sum of the first \\( n \\) terms of a geometric series, which can be rewritten as:\n",
    "\n",
    "\\[\n",
    "V(n) = \\sigma^2 \\mu^{n-1} \\cdot \\frac{1 - \\mu^n}{1 - \\mu} \\quad \\text{for} \\, \\mu \\neq 1.\n",
    "\\]\n",
    "\n",
    "Now, let's consider the different cases for \\( \\mu \\):\n",
    "\n",
    "### 1. Case 1: \\( \\mu > 1 \\)\n",
    "\n",
    "When \\( \\mu > 1 \\), the geometric series \\( 1 + \\mu + \\mu^2 + \\dots + \\mu^{n-1} \\) grows rapidly as \\( n \\) increases. In this case:\n",
    "\n",
    "- As \\( n \\to \\infty \\), \\( \\mu^n \\) grows larger, and the expression for \\( V(n) \\) becomes dominated by the term \\( \\mu^n \\) in the numerator. Thus, the variance grows without bound as \\( n \\) increases.\n",
    "  \n",
    "- More precisely, since the geometric series grows rapidly with \\( n \\), the variance \\( V(n) \\) increases exponentially with \\( n \\) if \\( \\mu > 1 \\). In the limit:\n",
    "\n",
    "\\[\n",
    "V(n) \\approx \\sigma^2 \\mu^{n-1} \\cdot \\frac{\\mu^n}{1 - \\mu} = \\frac{\\sigma^2 \\mu^{2n-1}}{1 - \\mu}.\n",
    "\\]\n",
    "\n",
    "So, as \\( n \\to \\infty \\), \\( V(n) \\to \\infty \\), and the variance becomes very large as the number of generations increases.\n",
    "\n",
    "### 2. Case 2: \\( \\mu = 1 \\)\n",
    "\n",
    "When \\( \\mu = 1 \\), the geometric series simplifies to:\n",
    "\n",
    "\\[\n",
    "V(n) = \\sigma^2 \\cdot 1^{n-1} \\cdot \\left( 1 + 1 + 1 + \\dots + 1 \\right) = \\sigma^2 \\cdot (n).\n",
    "\\]\n",
    "\n",
    "Thus, the variance grows linearly with \\( n \\), rather than exponentially:\n",
    "\n",
    "\\[\n",
    "V(n) = \\sigma^2 \\cdot n.\n",
    "\\]\n",
    "\n",
    "This means that if \\( \\mu = 1 \\), the variance increases steadily at a constant rate as the number of generations increases.\n",
    "\n",
    "### 3. Case 3: \\( \\mu < 1 \\)\n",
    "\n",
    "When \\( \\mu < 1 \\), the geometric series \\( 1 + \\mu + \\mu^2 + \\dots + \\mu^{n-1} \\) converges to a finite value as \\( n \\) increases. Specifically:\n",
    "\n",
    "- As \\( n \\to \\infty \\), the sum of the series approaches:\n",
    "\n",
    "\\[\n",
    "\\frac{1 - \\mu^n}{1 - \\mu} \\to \\frac{1}{1 - \\mu} \\quad \\text{as} \\, n \\to \\infty.\n",
    "\\]\n",
    "\n",
    "- The term \\( \\mu^{n-1} \\) decays to zero as \\( n \\) increases because \\( \\mu < 1 \\). Thus, the variance \\( V(n) \\) approaches a constant value as \\( n \\to \\infty \\):\n",
    "\n",
    "\\[\n",
    "V(n) \\to \\sigma^2 \\cdot \\frac{1}{1 - \\mu}.\n",
    "\\]\n",
    "\n",
    "In this case, the variance tends to a finite value as the number of generations increases. The variance doesn't grow without bound but instead reaches a limit as the branching process stabilizes.\n",
    "\n",
    "### Summary of Behavior Based on \\( \\mu \\):\n",
    "\n",
    "- **If \\( \\mu > 1 \\)**: The variance \\( V(n) \\) grows exponentially as \\( n \\to \\infty \\). The variance increases without bound.\n",
    "- **If \\( \\mu = 1 \\)**: The variance \\( V(n) \\) grows linearly with \\( n \\).\n",
    "- **If \\( \\mu < 1 \\)**: The variance \\( V(n) \\) approaches a finite value as \\( n \\to \\infty \\), and it stabilizes.\n",
    "\n",
    "Thus, the behavior of the variance depends significantly on the value of \\( \\mu \\), with exponential growth when \\( \\mu > 1 \\), linear growth when \\( \\mu = 1 \\), and stabilization at a finite value when \\( \\mu < 1 \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. The population goes extinct if Xn=0 for some n. Let us define the extinction time N by N=min{n: Xn=0}. Let F(n)=P(N<=n) be the CDF of the random variable N. Show that F(n)=Sump_k(F(n-1))^k, n=1,2,. Hint. Note that the event {N<=n} is the same as event {Xn=0}. Thus their probabilities are equal. Let k be the number of offspring of the original parent. The population becomes extinct at time n if and only if each of the k sub generations generated from the k offspring goes extinct in n-1 generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that the CDF of the extinction time \\( N \\), denoted by \\( F(n) = P(N \\leq n) \\), satisfies the recursive relation:\n",
    "\n",
    "\\[\n",
    "F(n) = \\sum_{k=0}^\\infty p_k (F(n-1))^k,\n",
    "\\]\n",
    "\n",
    "we will break down the problem step by step, using the hint provided.\n",
    "\n",
    "### 1. Extinction Time \\( N \\) and the Event \\( \\{N \\leq n\\} \\)\n",
    "\n",
    "Let \\( N \\) be the extinction time, defined as:\n",
    "\n",
    "\\[\n",
    "N = \\min\\{n : X_n = 0\\},\n",
    "\\]\n",
    "\n",
    "where \\( X_n \\) is the number of animals in the \\( n \\)-th generation. The event \\( \\{N \\leq n\\} \\) means that the population goes extinct at or before generation \\( n \\), i.e., \\( X_n = 0 \\).\n",
    "\n",
    "Thus, \\( F(n) = P(N \\leq n) \\) represents the probability that the population goes extinct by generation \\( n \\), which can be written as:\n",
    "\n",
    "\\[\n",
    "F(n) = P(X_n = 0).\n",
    "\\]\n",
    "\n",
    "The extinction time \\( N \\) occurs at generation \\( n \\) if and only if the population survives up to generation \\( n-1 \\), but goes extinct at generation \\( n \\). This condition can be understood by considering the offspring process and the branching structure.\n",
    "\n",
    "### 2. Recursive Structure of the Problem\n",
    "\n",
    "We know that each animal in generation \\( n-1 \\) produces a random number \\( Y_i \\) of offspring according to the distribution \\( p_k = P(Y = k) \\). Let \\( k \\) be the number of offspring of the original animal. For the population to go extinct at generation \\( n \\), it is required that each of the \\( k \\) offspring goes extinct at or before generation \\( n-1 \\).\n",
    "\n",
    "Hence, the population goes extinct at generation \\( n \\) if:\n",
    "\n",
    "- The initial animal has \\( k \\) offspring.\n",
    "- Each of these \\( k \\) offspring goes extinct by generation \\( n-1 \\).\n",
    "\n",
    "Therefore, the probability that the population goes extinct at generation \\( n \\) given that the initial animal has \\( k \\) offspring is \\( (F(n-1))^k \\), because each of the \\( k \\) offspring independently has a probability of \\( F(n-1) \\) to go extinct by generation \\( n-1 \\).\n",
    "\n",
    "### 3. Summing Over All Possible Offspring Numbers\n",
    "\n",
    "Since the number of offspring \\( k \\) is random and follows the distribution \\( p_k = P(Y = k) \\), the probability that the population goes extinct at or before generation \\( n \\) is the weighted sum over all possible values of \\( k \\). For each \\( k \\), the probability of extinction at generation \\( n \\) is \\( (F(n-1))^k \\), and the probability that the original animal has exactly \\( k \\) offspring is \\( p_k \\).\n",
    "\n",
    "Thus, the total probability that the population goes extinct at or before generation \\( n \\) is:\n",
    "\n",
    "\\[\n",
    "F(n) = \\sum_{k=0}^\\infty p_k (F(n-1))^k.\n",
    "\\]\n",
    "\n",
    "### 4. Conclusion\n",
    "\n",
    "We have shown that the CDF of the extinction time \\( N \\), denoted by \\( F(n) \\), satisfies the recursive relation:\n",
    "\n",
    "\\[\n",
    "F(n) = \\sum_{k=0}^\\infty p_k (F(n-1))^k.\n",
    "\\]\n",
    "\n",
    "This recursive formula captures the idea that the extinction event for the population depends on the extinction of the subpopulations generated by each offspring of the initial animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Suppose the p0=1/4 p1=1/2 p2=1/4. use the formula that you found in d. to compute the CDF F(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the probabilities \\( p_0 = \\frac{1}{4} \\), \\( p_1 = \\frac{1}{2} \\), and \\( p_2 = \\frac{1}{4} \\), we can now compute the CDF \\( F(n) = P(N \\leq n) \\) using the recursive formula that we derived in part D:\n",
    "\n",
    "\\[\n",
    "F(n) = \\sum_{k=0}^\\infty p_k (F(n-1))^k.\n",
    "\\]\n",
    "\n",
    "We will use this formula to compute \\( F(n) \\) for different values of \\( n \\). To begin, we need to set the initial condition. The population starts with one animal, and we assume that \\( F(0) = 0 \\), because the population cannot go extinct at generation 0 (there's at least one animal initially).\n",
    "\n",
    "### Step-by-Step Calculation\n",
    "\n",
    "**For \\( n = 1 \\):**\n",
    "\n",
    "\\[\n",
    "F(1) = p_0 (F(0))^0 + p_1 (F(0))^1 + p_2 (F(0))^2.\n",
    "\\]\n",
    "\n",
    "Since \\( F(0) = 0 \\), this simplifies to:\n",
    "\n",
    "\\[\n",
    "F(1) = p_0 (0)^0 + p_1 (0)^1 + p_2 (0)^2 = \\frac{1}{4} \\cdot 1 + \\frac{1}{2} \\cdot 0 + \\frac{1}{4} \\cdot 0 = \\frac{1}{4}.\n",
    "\\]\n",
    "\n",
    "Thus, \\( F(1) = \\frac{1}{4} \\).\n",
    "\n",
    "**For \\( n = 2 \\):**\n",
    "\n",
    "\\[\n",
    "F(2) = p_0 (F(1))^0 + p_1 (F(1))^1 + p_2 (F(1))^2.\n",
    "\\]\n",
    "\n",
    "Substitute \\( F(1) = \\frac{1}{4} \\):\n",
    "\n",
    "\\[\n",
    "F(2) = \\frac{1}{4} (F(1))^0 + \\frac{1}{2} (F(1))^1 + \\frac{1}{4} (F(1))^2 = \\frac{1}{4} \\cdot 1 + \\frac{1}{2} \\cdot \\frac{1}{4} + \\frac{1}{4} \\cdot \\left( \\frac{1}{4} \\right)^2.\n",
    "\\]\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "\\[\n",
    "F(2) = \\frac{1}{4} + \\frac{1}{8} + \\frac{1}{4} \\cdot \\frac{1}{16} = \\frac{1}{4} + \\frac{1}{8} + \\frac{1}{64}.\n",
    "\\]\n",
    "\n",
    "To combine these terms, we use a common denominator of 64:\n",
    "\n",
    "\\[\n",
    "F(2) = \\frac{16}{64} + \\frac{8}{64} + \\frac{1}{64} = \\frac{25}{64}.\n",
    "\\]\n",
    "\n",
    "Thus, \\( F(2) = \\frac{25}{64} \\).\n",
    "\n",
    "**For \\( n = 3 \\):**\n",
    "\n",
    "\\[\n",
    "F(3) = p_0 (F(2))^0 + p_1 (F(2))^1 + p_2 (F(2))^2.\n",
    "\\]\n",
    "\n",
    "Substitute \\( F(2) = \\frac{25}{64} \\):\n",
    "\n",
    "\\[\n",
    "F(3) = \\frac{1}{4} (F(2))^0 + \\frac{1}{2} (F(2))^1 + \\frac{1}{4} (F(2))^2 = \\frac{1}{4} \\cdot 1 + \\frac{1}{2} \\cdot \\frac{25}{64} + \\frac{1}{4} \\cdot \\left( \\frac{25}{64} \\right)^2.\n",
    "\\]\n",
    "\n",
    "First, calculate \\( \\left( \\frac{25}{64} \\right)^2 = \\frac{625}{4096} \\), then compute:\n",
    "\n",
    "\\[\n",
    "F(3) = \\frac{1}{4} + \\frac{1}{2} \\cdot \\frac{25}{64} + \\frac{1}{4} \\cdot \\frac{625}{4096}.\n",
    "\\]\n",
    "\n",
    "Simplifying each term:\n",
    "\n",
    "\\[\n",
    "F(3) = \\frac{1}{4} + \\frac{25}{128} + \\frac{625}{16384}.\n",
    "\\]\n",
    "\n",
    "To combine these, use a common denominator of 16384:\n",
    "\n",
    "\\[\n",
    "F(3) = \\frac{4096}{16384} + \\frac{3200}{16384} + \\frac{625}{16384} = \\frac{7921}{16384}.\n",
    "\\]\n",
    "\n",
    "Thus, \\( F(3) = \\frac{7921}{16384} \\).\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "The CDF values for \\( F(n) \\) are as follows:\n",
    "\n",
    "- \\( F(0) = 0 \\) (initial condition),\n",
    "- \\( F(1) = \\frac{1}{4} \\),\n",
    "- \\( F(2) = \\frac{25}{64} \\),\n",
    "- \\( F(3) = \\frac{7921}{16384} \\).\n",
    "\n",
    "This recursive method allows you to compute the CDF \\( F(n) \\) for any \\( n \\), and the extinction probability grows as \\( n \\) increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. show that if i is a recurrent state i<->j then j is a recurrent state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that if state \\( i \\) is recurrent and there exists a path between states \\( i \\) and \\( j \\) (i.e., \\( i \\leftrightarrow j \\)), then state \\( j \\) must also be recurrent, we can proceed step by step:\n",
    "\n",
    "### Definitions\n",
    "\n",
    "- **Recurrent State**: A state \\( i \\) is recurrent if, starting from \\( i \\), the process will return to \\( i \\) with probability 1, i.e., the probability of ever returning to state \\( i \\) is 1.\n",
    "  \n",
    "  More formally, state \\( i \\) is recurrent if:\n",
    "  \\[\n",
    "  P(\\text{return to } i \\mid \\text{start at } i) = 1\n",
    "  \\]\n",
    "\n",
    "- **Transient State**: A state \\( i \\) is transient if, starting from \\( i \\), there is a non-zero probability that the process will never return to \\( i \\).\n",
    "\n",
    "- **Communication Between States**: States \\( i \\) and \\( j \\) communicate, denoted as \\( i \\leftrightarrow j \\), if it is possible to reach \\( j \\) from \\( i \\) and vice versa. In other words:\n",
    "  - \\( i \\) can reach \\( j \\) (i.e., there exists a path from \\( i \\) to \\( j \\)).\n",
    "  - \\( j \\) can reach \\( i \\) (i.e., there exists a path from \\( j \\) to \\( i \\)).\n",
    "\n",
    "### Proof\n",
    "\n",
    "Now, let us assume the following:\n",
    "- State \\( i \\) is recurrent.\n",
    "- \\( i \\leftrightarrow j \\) (i.e., there exists a path from \\( i \\) to \\( j \\) and from \\( j \\) to \\( i \\)).\n",
    "\n",
    "We aim to show that \\( j \\) is also recurrent.\n",
    "\n",
    "#### Step 1: Path from \\( i \\) to \\( j \\)\n",
    "Since \\( i \\leftrightarrow j \\), there is a path from \\( i \\) to \\( j \\). This means that if we start at state \\( i \\), there is a non-zero probability of reaching state \\( j \\) at some time.\n",
    "\n",
    "#### Step 2: Return from \\( j \\) to \\( i \\)\n",
    "Similarly, because \\( i \\leftrightarrow j \\), there is also a path from \\( j \\) to \\( i \\). Hence, if we start at state \\( j \\), there is a non-zero probability of eventually reaching state \\( i \\).\n",
    "\n",
    "#### Step 3: Recurrence of \\( i \\)\n",
    "By assumption, state \\( i \\) is recurrent. Therefore, starting from \\( i \\), the process will eventually return to \\( i \\) with probability 1.\n",
    "\n",
    "#### Step 4: Recurrent path to \\( j \\)\n",
    "Since there is a path from \\( i \\) to \\( j \\) and from \\( j \\) to \\( i \\), we can imagine the following scenario:\n",
    "- Starting at state \\( i \\), the process will return to \\( i \\) with probability 1.\n",
    "- Since there is a path from \\( i \\) to \\( j \\), and from \\( j \\) back to \\( i \\), we will eventually visit state \\( j \\) at least once (because we must visit all states in a recurrent cycle).\n",
    "\n",
    "#### Step 5: Returning to \\( j \\)\n",
    "Once the process reaches \\( j \\), we can return to \\( i \\), and since \\( i \\) is recurrent, we know the process will eventually return to \\( i \\) and repeat the cycle.\n",
    "\n",
    "This shows that starting from \\( j \\), there is a non-zero probability that the process will return to \\( j \\). More formally, the probability of returning to \\( j \\) is 1 because of the recurrence of \\( i \\) and the paths between \\( i \\) and \\( j \\).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Since state \\( i \\) is recurrent, and there exists a path from \\( i \\) to \\( j \\) and from \\( j \\) to \\( i \\), it follows that state \\( j \\) is also recurrent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let P = rows (0,1), (1,0) show that stationary distribution is (1/2, 1/2) . does this chain converge? why, why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
